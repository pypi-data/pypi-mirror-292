num1:
    text: |-
        ### **1)** Линейная модель множественной регрессии. Основные предпосылки метода наименьших квадратов**.**

        Линейная модель линейной связи – это уравнение связи с несколькими переменными  $y = f(x_1, x_2, ... x_n) + ξ$ , где y – зависимая переменная, $x_n$  – независимая переменная. $y = a_0 + a_1x_1+...+a_nx_n$  – формула линейной множественной регрессии, где   – это параметры, которые вычисляются разными способами, один из них – метод наименьших квадратов.

        Также данная модель может записываться в матричном виде Y = XB+E, где Y – столбец зависимых переменных, X – матрица факторов, B – вектор коэффициентов, Е – вектор случайных ошибок. В данном случае коэффициенты матрицы B так же могут вычисляться с помощью МНК.

        МНК - математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений некоторых функций от экспериментальных входных данных.

        Предпосылки:

        1. *Математическое ожидание случайного отклонения ε равно нулю: Е(ε)=0 для всех наблюдений*
        2. *Дисперсия случайных отклонений ε постоянна:*  *для любых наблюдений i и j (гомоскедастичность)*
        3. *Случайные отклонения $\varepsilon_i$ и $\varepsilon_j$ ( $i \neq j$) не коррелируют между собой (отсутствие автокорреляции). $E(\varepsilon_i, \varepsilon_j ) = 0, i \neq j$*
        4. *Случайные отклонения должны быть статистически независимы (некоррелированы) от объясняющих переменных.*
        5. *Отсутствие мультиколлинеарности. Между объясняющий переменными отсутствует сильная линейная зависимость.*
        6. *Возмущения $\varepsilon_i$ нормально распределены $\varepsilon_i \sim N(0; \sigma^2)$.*

        Если регрессионная модель удовлетворяет условиям Гаусса-Маркова, ее параметры обладают свойствами несмещенности, эффективности и состоятельности.

        Оценки в матричном виде могут быть рассчитаны по как $B=(X^TX)^{-1}X^TY$

        Если уравнение представлено не матрицей, коэффициенты рассчитываются из системы производных по каждому коэффициенту.
    keywords: [1, линейная, множественной регрессии, предпосылки, наименьших квадратов]
num2:
    text: |-
        ### **2)** Нелинейные модели регрессии. Походы к оцениванию. Примеры

        Различают два класса нелинейной регрессии:

        1. Нелинейные по переменным, но линейные по параметрам. Примеры: полиномы > 1 степени, гиперболы. Например, полиномиальная модель: $y = \beta_0 + \beta_1x + \beta_2x^2 + … + \beta_m*x^m + \varepsilon$
        2. Нелинейные по параметрам но линейные по переменным. Примеры: Степенные, показательные функции. Например, степенная модель: $y = \beta_0*x_1^{\beta_1} * x_2^{\beta_2} * … x_m^{\beta_m}\varepsilon$
        Так как классическая модель регрессии предполагает линейный характер зависимости переменных Y и X, то перед применением МНК для оценки, нелинейные модели по параметрам линеаризовывают, это делается путём логарифмирования правой и левой части. Нелинейные по переменным же модели требуют добавления новых объясняющих переменных соответсвующим образом преобразованных. Для оценки параметров нелинейных моделей используют два подхода: Первый подход основан на линеаризации модели и заключается в том, что с помощью подходящих преобразований исходных переменных исследуемую нелинейную зависимость представляют в виде линейной между преобразованными переменными. Второй подход, основанный на применении методов нелинейной оптимизации, применяется в том случає, когда подобрать соответствующее преобразование не удается.

        Например: полиномиальная модель m-го порядка может быть приведена к линейному виду путем замены переменных $x^j = x_j^{\prime}$. После замены переменных получим линейную регрессионную модель вида $y = \beta_0 + \sigma_{j=1}^m \beta_jx_j^{\prime} + \varepsilon$
        К классу степенных функций относятся кривые спроса и предложения, производственные функции и тд. Например, производственная функция Кобба-Дугласа $Y = AK^{\alpha}L^{\beta}\varepsilon$
    keywords: [2, нелинейные, подходы к оцениванию ]
num3:
    text: |-
        ### **3)** Тестирование правильности выбора спецификации: типичные ошибки спецификации модели, Тест Рамсея (тест RESET), условия применения теста.

        Типичные ошибки:

        1. Неверно выбран тип уравнения регрессии
        2. В линейное уравнение множественной регрессии включен несущественный регрессор
        3. В линейное уравнение множественной регрессии не включен существенный регрессор

        Тест Рамсея:
        RESET – тест Рамсея отвечает на вопрос, надо ли включать в регрессию степени независимых переменных (идея заключается в том, что добавление нелинейных функций $\hat Y$ не должно улучшать наших знаний относительно Y)
        Условие применения: модель - линейное уравнение множественной регрессии

        $H_0:$ спецификация модели является правильной

        $H_1:$ спецификация неправильная (не включён существенный признак)

        Для проверки сначала оценивается исходная модель, далее заново оценивается эта же модель но с добавлением вычисленных на предыдущем этапе предсказываемых значений в степени p (обычно не большой). Далее происходит оценка модели через F статистику, если она больше критического значения, значит отсутствует важный признак в модели.

        $F_{набл} = \frac{\frac{(ESS_1 - ESS_2}{p}}{\frac{ESS_2}{n-m-p}} \sim F_{крит} (p, n-m-p), 
        F_{набл} \gt F_{крит} (p, n-m-p)$
    keywords: [спецификации, Рамсея]
num4:
    text: |-
        ### **4)** Тестирование правильности выбора спецификации: типичные ошибки спецификации модели, Критерий Акаике, Критерий Шварца. условия применения критериев.

        Типичные ошибки:

        1. Неверно выбран тип уравнения регрессии
        2. В линейное уравнение множественной регрессии включен несущественный регрессор
        3. В линейное уравнение множественной регрессии не включен существенный регрессор

        AIC позволяет сравнивать несколько статистических моделей друг с другом для того, чтобы определить, какая из моделей лучше соответствует данным. Особенностью критерия является введение штрафа за число параметров модели.
        Статистика рассчитывается как: $AIC = ln(\frac{ESS_k}{n})+\frac{2k}{n}+1+ln(2\pi)$. При увеличении объясняющих переменных первое слагаемое в правой части уменьшается, второе – увеличивается.

        BIC - мера относительного качества моделей, учитывающая степень «подгонки» модели под данные со штрафом на используемое количество оцениваемых параметров. То есть критерий основаны на неком компромиссе между точностью и сложностью модели.

        Статистика: $BIC = ln(\frac{ESS_k}{n})+\frac{kln(n)}{n}+1+ln(2\pi)$*.* При увеличении количества объясняющих переменных первое слагаемое в правой части уменьшается, а второе – увеличивается.

        В обоих случаях предпочтение отдаётся тем моделям, где данные статистики меньше. Различаются они способом оценки баланса между переменными и ESS.

        Условия применения критериев:
        1. Сравниваемые модели построены на одном и том же наборе данных.
        2. Сравниваемые модели имеют одну и ту же объясняемую переменную.
        3. Обучающая выборка имеет бесконечный размер.
    keywords: [Акаике, Шварца]
num5:
    text: |-
        ### **5)** Гетероскедастичность: определение, причины, последствия. Тест Голдфеда-Квандта и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:
        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:
        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$ будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Голфеда-Квандта:
        $H_0$: Присутствует гомоскедастичность
        $H_1$: Присутствует гетероскедастичность

        1. Выборочные данные упорядочиваются по величине модуля регрессора, относительно которого есть подозрение на гетероскедастичность
        2. Выборка делится на 3 группы, где $n_1 = n_3, k <n_1<\frac{n}{2}$, k – число параметров, n – общая выборка
        3. Далее оцениваются две вспомогательные регрессии по 1 и 3 группе по которым вычисляются суммы квадратов остатков $ESS_1, ESS_3$
        4. Далее вычисляются статистики $GQ=\frac{ESS_1}{ESS_3}, GQ^{-1}=\frac{ESS_3}{ESS_1}$
        5. Определяется F распределение $F(\alpha, n_1-k, n_1-k)$
        6. Если $GQ \le F_{кр}$ и $GQ^{-1} \le F_{кр}$ то присутствует гомоскедастичность
        Данный тест применяется при маленьких выборках и нормально распределённых остатках, которые не коррелируют друг с другом, cтандартные отклонения $\sigma^2_{e_i} $пропорциональны значениям $x_i$.
    keywords: [Гетероскедостичность, Голдфеда-Квандта]
num6:
    text: |-
        ### **6)** Гетероскедастичность: определение, причины, последствия. Тест ранговой корреляции Спирмена и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:
        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:
        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Спирмена:
        $H_0$: Присутствует гомоскедастичность
        $H_1$: Присутствует гетероскедастичность
        1. Рассчитываются абсолютные значения остатков модели
        2. Остатки и значения фактора ранжируются (определяются их ранги)
        3. Рассчитывается ранговое значение $D^2_i = (D_x + D_e)^2$ и вычисляется сумма всех этих квадратов
        4. Далее определяется коэффициент ранговой корреляции $r = 1 - \dfrac{6\sum_{i=1}^nD^2_i}{n(n^2 -1)}$
        5. Определяется критическое значение по Стьюденту t(a, n-2)
        6. Вычисляется t статистика $t = \dfrac{r\sqrt{n-2}}{\sqrt{1-r^2}} *\sqrt{n-2}$
        7. Если t < tкр (a=0,05; n=2), то гомоскедастичность, иначе гетеро
        Данный тест рекомендуется использовать при равномерном распределении измененяемой величины.
    keywords: [Гетероскедостичность, Спирмена]
num7:
    text: |-
        ### **7)** Гетероскедастичность: определение, причины, последствия. Тест Тест Бреуша-Пагана и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:
        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:
        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Бреуша-Пагана:
        $H_0$: Присутствует гомоскедастичность
        $H_1$: Присутствует гетероскедастичность
        1. Рассчитываем обычную регрессию через МНК и вычисляем остатки, квадраты остатков
        2. Оценка дисперсии возмущений $\hat \sigma^2 = \dfrac{\sum_{t=1}^ne^2_t}{n}$
        3. Оценивается новая регрессия вида $\dfrac{e^2_t}{\hat \sigma_t^2} = \gamma_0 + Z_t^T \cdot \gamma +v_t$ и находится RSS
        4. Вычисляется статистика $BP=\dfrac{RSS}{2}$
        5. Если $BP < \chi^2(k)$, то гомоскедастичность, иначе гетеро
        Данный метод чаще всего использую при больших выборках
    keywords: [Гетероскедастичность,Бреуша-Пагана]
num8:
    text: |-
        ### **8)** Гетероскедастичность: определение, причины, последствия. Тест Глейзера и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:
        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:
        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Глейзера:
        $\sigma_i = a_0 + a_1*x_i^{\gamma} +w_i$

        $H_0$: Присутствует гомоскедастичность (a_1=0)

        $H_1$: Присутствует гетероскедастичность $(a1 \neq 0)$

        1. Вычисляем абсолютные значения остатков для обычной модели при МНК
        2. Определяем $t_{кр} = t(\alpha, n-2)$
        3. Строим МНК оценки для моделей $|\varepsilon_i|=a_0 +a_1 \cdot x_i^{\gamma}$, для $\gamma$ от -1 до 1 с шагом 0.5, если хотя бы для одной модели t статистика a1 оказалась значимость, значит есть гетероскедастичность, иначе нет

        Данный тест способен не только выявить гетероскедастичность, но и конкретный вид зависимости этих остатков от каждой независимой переменной
    keywords: [Гетероскедастичность, Глейзера]
num9:
    text: |-
        ### **9)** Способы корректировки гетероскедастичности: взвешенный метод наименьших квадратов (ВМНК) и особенности его применения.

        Способы:
        1. Взвешенный метод наименьших квадратов (ВМНК)
        2. Доступный взвешенный метод наименьших квадратов (ДВМНК)
        3. Обобщённый метод наименьших квадратов (ОМНК)

        Перечисленные методы нацелены на преобразование переменных таким образом, чтобы в спецификации преобразованной модели случайное возмущение удовлетворяло предпосылкам Гаусса-Маркова

        ВМНК:
        Применяется, если бы нам были известны дисперсии всех ошибок.

        Правая и левая часть уравнения регрессии $y_i = \beta_0 + \beta_1*x_i + \varepsilon_i$ делятся на известное $\sigma_i$. 

        Модель примет вид:
        $$
        \dfrac{Y_i}{\sigma_i}=\beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{X_{2i}}{\sigma_i}+...+\beta_k \dfrac{X_{ki}}{\sigma_i}+\dfrac{u_i}{\sigma_i}
        $$

        Её можно привести к обычному линейному виду заменив новые коэффициенты:
        $$
        Y^*_i=\beta_0 z_i + \beta_1 X^*_{2i}+...+\beta_kX^*_{ki}+u^*_{i}  
        $$

        Данный вектор случайных отклонений $u_i^*$ удовлетворяет условию гомоскедастичности.
        В случае неизвестных дисперсий положим $\sigma_i = \sqrt{X_i}$ и проделаем всё то же самое
    keywords: [корректировки гетероскедастичности, ВМНК] 
num10:
    text: |-
        ### **10) Автокорреляция: определение, причины, последствия. Тест Дарбина-Уотсона и особенности его применения.**

        Автокорреляция – это наличие сильной корреляционной зависимости между последовательными уровнями ряда динамики.

        Автокорреляция — это понятие математической статистики, которое характеризует степень статистической взаимосвязи между элементами данных одного временного ряда.

        **Причинами автокорреляции являются:**
        - ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);
        - ошибки измерений*;*
        - характер наблюдений (например, данные временных рядов).
        - Инертность экономических показателей
        - The Cobweb effect (паутинообразный эффект) (во многих отраслях производства экономические показатели реагируют на изменение экономических условий с запаздыванием);
        - «Манипулирование данными». Сглаживание данных.

        **Последствия автокорреляции:**
        - Хотя оценки МНК коэффициентов регрессии останутся несмещенными, они уже не будут эффективными
        - Оценки МНК для стандартных отклонений коэффициентов регрессии будут смещенными, чаще всего вниз, т.е будут заниженными.
        - Статистики t и F будут неадекватными. Следствием заниженности оценок стандартных отклонений коэффициентов является завышенность t – статистик.

        **Тест Дарбина-Уотсона:**
        $H_0$: между остатками нет корреляции.

        $H_1$: остатки автокоррелированы.

        Формула для вычисления статистики Дарбина-Уотсона **DW** по остаткам регрессии:
        $$
        DW = \frac {\sum_{t=2}^{n}(e_t - e_{t-1})^2}{\sum_{t=1}^{n}e_t^2} 
        $$

        **Предпосылки:**
        - случайное возмущение $\varepsilon$  распределено нормально;
        - не подвержено гетероскедастичности;
        - модель не включает лаговые значения эндогенных переменных.
        При $DW \rightarrow 2$, автокорреляции нет.

        При $DW \rightarrow 0$, положительная автокорреляции.

        При $DW \rightarrow 4$, отрицательная автокорреляции.

        На практике проверка гипотезы **$H_0$**  об отсутствии автокорреляции остатков осуществляется с помощью сравнения статистики **DW** с теоретическими значениями **$d_l$** и **$d_u$** для заданного числа наблюдений n, числа независимых переменных модели k и уровня значимости α:

        - 0 < *DW* < *$d_l$* - гипотеза $H_0$  отвергается, есть положительная автокорреляция;
        - *dl* < *DW* < *$d_u$* - зона неопределённости;
        - $*d_u*$ < *DW* < **4 - $*d_u*$ - гипотеза $H_0$  не отвергается, автокорреляции нет;
        - 4 - $*d_u*$ < *DW* < **4 - $*d_l*$ - зона неопределённости;
        - 4 - $*d_l*$ < *DW* < **4 - гипотеза $H_0$ отвергается, есть отрицательная автокорреляция.

        **При использовании теста *DW* следует учитывать следующие ограничения:**

        - он применим лишь для модели с ненулевым свободным членом;
        - остатки должны описываться авторегрессионной моделью первого порядка;
        - регрессоры являются нестохастическими;
        - модель не подвержена гетероскедастичности;
        - применяется для выявления автокорреляции только между регрессионными остатками в последовательных наблюдениях.
    keywords: [Дарбина-Уотсона, Автокорреляция ]
num61:
    text: |-
        ### 61) Адаптивная модель прогнозирования Брауна.

        Адаптивные модели прогнозирования — это модели, способные приспосабливать свою структуру и параметры **к изменению свойств моделируемого процесса**. Как и в трендовых моделях, основным фактором в адаптивных моделях является время, но наблюдениям (уровням ряда) придаются различные веса в зависимости **от силы их влияния на текущий уровень ряда.** Это позволяет учитывать изменения в тенденции ряда, а также колебания.
        Все адаптивные модели основаны на двух схемах: **скользящего среднего
        и авторегрессии**. В моделях скользящего среднего текущий уровень является средневзвешенной суммой всех предыдущих уровней, причем весовые коэффициенты убывают по мере удаления от текущего уровня. Такие модели хорошо отражают изменение тенденции, но **не позволяют отражать колебания.** В авторегрессионных моделях для расчета текущего уровня используются не все, а только несколько последних значений ряда, при этом значения весовых коэффицентов определяются не их близостью к моделируемому уровню, а **теснотой связи между уровнями.**
        Наиболее часто для краткосрочного прогнозирования изменяющихся процессов используется адаптивная модель Брауна. Она позволяет о**тображать развитие линейной или параболической тенденции, а также рядов без тенденции.**
        Соответственно различают модели нулевого (наивная), первого или второго порядков вида:

        $y_{t+k} = A_0$

        $y_{t+ k} = A_0 + A_1k$

        $y_{t + k} = A_0 + A_1k + A_2k^2$

        где t — текущее время; к — время упреждения.
        Порядок модели определяется априорно из предварительного анализа вре-
        менного ряда и законов развития прогнозируемого процесса.
        Модель первого порядка строится следующим образом:

        По нескольким первым точкам методом наименьших квадратов найдем
        значения параметров $А_0, А_1$ линейной модел (или зададим их):
        $\hat{y} _{прогноз}(t) = A_0 + A_1t$
        Используя найденные параметры, найдем прогнозное значение на следующем шаге:
        $\hat{y}_{прогноз} (t + k) = A_0(t) + A_1(t)k, k = 1.$
        Найдем ошибку прогнозирования:
        $e (t + k) = y (t + k) - \hat{y}_{прогноз} (t + k)$ 
        В соответствии с ошибкой изменим значения параметров модели:
        $A_0(t + 1) = A_0(t) + A_1 (t) + (1 - \beta)^2 e(t)$
        $A_1 (t + 1) = A_1 (t) + (1 - \beta )^2 e(t)$
        где В - коэффициент дисконтирования данных, 0 < B < 1.
        Eсли t < N (т. е. время обучения модели еще не заверши-лось), при t ≥ N будем использовать полученное значение как прогнозное, не изменяя параметров модели.
        Дополним точечный прогноз интервальным:
        $U(k) = S_{\hat{y}} * t_{\alpha} * \sqrt{1 + \frac{1}{n} + \frac{3*(n + 2k -1)^2}{n(n^2 - 1)}}$
        где $t_a$ — значение критерия Стьюдента; $Sy$ — среднеквадратичное отклонение прогнозируемого показателя; п — число наблюдений ряда.
    keywords: [61, модель прогнозирования Брауна]
num62:
    text: |-
        ### 62) Моделирование тренд-сезонных процессов. Типы функциональных зависимостей.

        Получаемый тренд сглаживают скользящей средней по пяти точкам и получают новую оценку тренда $f_{ij}^2$. Чтобы не потерять точки в начале и в конце ряда их сглаживают по трем точкам, причем для крайних точек используют специальные формулы сглаживания:

        Чаще всего на практике приходится иметь дело с рядами, включающими три компоненты - тренд, сезонную и случайную составляющие. Такие процессы принято называть тренд-сезонными. В зависимости от вида связи между компонентами может быть построена либо **аддитивная модель**:
        $Y_t = T + S+ e$

        либо **мультипликативная модель** временного ряда:
        $y_t = T * S * e$
        Довольно часто используется **модель смешанного типа:**
        $y_t = T • S + e$
        Выбор вида модели осуществляется на основе **анализа структуры сезонных колебаний.** Если амплитуда колебаний приблизительно постоянна, строят аддитивную модель временного ряда, если амплитуда колебаний изменяется, то строят мультипликативную модель временного ряда.
        Для выделения компонент тренд-сезонных временных рядов используют итерационные методы фильтрации, основная идея которых заключается в многократном применении скользящей средней и одновременной оценке сезонной волны на каждом шаге. Итерационные методы отличаются простотой и удовле-творительной чистотой фильтрации компонент ряда, однако применение скользящей средней приводит к потере части информации на концах временного ряда.
    keywords: [62, тренд-сезонных процессов]
num63:
    text: |-
        ### 63) Модель Хольта-Уинтерса (адаптивная модель).

        Идея этого метода заключается в добавлении еще одной, третьей, компоненты — сезонности. Соответственно, метод применим только в случае, **если ряд этой сезонностью не обделён**. Сезонная компонента в модели будет объяснять повторяющиеся колебания вокруг уровня и тренда, а характеризоваться она будет длиной сезона — периодом, после которого начинаются повторения колебаний. Для каждого наблюдения в сезоне формируется своя компонента, например, если длина сезона составляет 7 (например, недельная сезонность), то получим 7 сезонных компонент. Hовую систему:
        $l_x = \alpha(y_x - s_{x - L}) + (1 - \alpha)(l_{x-1} + b_{x-1})$

        $b_x = \beta(l_x - l_{x-1}) + (1 - \beta)b_{x-1}$

        $s_x = \gamma(y_x - l_x) + (1-\gamma) s_{x-L}$

        $\hat{y}_{x+m} = l_m + mb_x + s_{x - L + 1 + (m - 1) mod L}$

        Уровень теперь зависит от текущего значения ряда за вычетом соответ-ствующей сезонной компоненты, тренд остаётся без изменений, а сезонная компонента зависит от текущего значения ряда за вычетом уровня и от предыдущего значения компоненты. Теперь, имея сезонную компоненту, мы можем предсказывать уже не на один, и даже не на два, а на произвольные т шагов вперёд, что не может не радовать.
        Построение модели тройного экспоненциального сглаживания, также из-вестного по фамилиям её создателей — Чарльза Хольта и его студента Питера Уинтерса:
        $\hat{y}_{max_x} = l_{x-1}+ b_{x -1 } + s_{x - T} + m*d_{t - T}$

        $\hat{y}{max_x} = l{x-1}+ b_{x -1 } + s_{x - T} - m*d_{t - T}$

        $d_t = \gamma|y_t - \hat{y}_t| + (1 - \gamma)d_{t-T}$
        где Т — длина сезона, d — предсказанное отклонение, а остальные параметры берутся из тройного сглаживания.

        либо мультипликативная модель временного ряда:
        (11.61)
        Довольно часто используется модель смешанного типа:
        Vt = T • S + E
        (11.62)
        Выбор вида модели осуществляется на основе анализа структуры сезонных колебаний. Если амплитуда колебаний приблизительно постоянна, строят аддитивную модель временного ряда, если амплитуда колебаний изменяется, то строят мультипликативную модель временного ряда.
        Для выделения компонент тренд-сезонных временных рядов используют итерационные методы фильтрации, основная идея которых заключается в многократном применении скользящей средней и одновременной оценке сезонной волны на каждом шаге. Итерационные методы отличаются простотой и удовле-творительной чистотой фильтрации компонент ряда, однако применение сколь-зящей средней приводит к потере части информации на концах временного ряда.
    keywords: [63, Модель Хольта-Уинтерса, адаптивная модель]
num65:
    text: |-
        ## 65) Модель Тейла-Вейджа (мультипликативная модель).

        Эта трехпараметрическая трендсезонная модель, сочетающая линейный рост с аддитивной сезонностью.

        $$
        \begin{equation}
        y_t(t) = a_{1,t} + g_t + u_t \quad (11.73)
        \end{equation}
        $$

        Где $a_{1,t} = a_{1,t-1} + a_{2,t}$ — величина уровня ряда после удаления сезонных колебаний; 

        $a_{1,t}$ — аддитивный коэффициент роста, 

        $g_t$ — аддитивный коэффициент сезонности, 

        $u_t$ — белый шум.

        Прогноз по этой модели на $\tau$ шагов вперед определяется выражением:

        $$
         \begin{equation}
        \hat{y}_{\tau}(t) = \hat{a}_{1,t} + \hat{a}_{2,t} \cdot \tau + \hat{g}_{t-l+\tau} \quad (11.74)
        \end{equation}
        $$

        Обновление коэффициентов осуществляется следующим образом:

        $$
        \begin{equation}
        \hat{a}_{1,t} = \alpha_1 \cdot (y_t - \hat{g}_{t-l}) + (1 - \alpha_1) \cdot (\hat{a}_{1,t-1} + \hat{a}_{2,t-1})
        \end{equation}
        $$

        $$
        \begin{equation}
        \hat{g}_t = \alpha_2 \cdot (y_t - \hat{a}_{1,t}) + (1 - \alpha_2) \cdot \hat{g}_{t-l} \quad (11.75)
        \end{equation}
        $$

        $$
        \begin{equation*}
        0 < \alpha_1, \alpha_2, \alpha_3 < 1
        \end{equation*}
        $$

        где $a_{1,t}$ — характеристика тенденции развития;

        $g_1, g_{t-1}, \ldots, g_{t-l+1}$ — аддитивный сезонный фактор; 

        $l$ — число фаз в полном сезонном цикле (для ежемесячных наблюдений $l = 12$, для квартальных $l = 4$).
    keywords: [65, Модель Тейла-Вейджа, мультипликативная модель]
num66:
    text: |-
        ## 66) Метод Четверикова.

        Метод Четверикова используется для выявления отдельных компонент временного ряда

        1. Исходный ряд $y_t$ выравнивается во формуле среднехронологиской с периодом 1 год, т.е. Т=12. Не выровненные значения в начале и конце ряда отбрасывают. Получают предварительную оценку тренда:

        
        $$
        \tilde{y}_t = f'_t
        $$
        
        и вычисляют отклонения исходного ряда от выровненного:
        
        $$
        \begin{equation}
        \hat{a}_{2,t} = \alpha_3 \cdot (\hat{a}_{1,t} - \hat{a}_{1,t-1}) + (1 - \alpha_3) \cdot \hat{a}_{2,t-1}
        \end{equation}
        $$
        
        $$
        l_{it} = Y_{it}-f'_{tj}
        $$
        
        2. Для каждого года i вычисляется среднеквадратическое отклонение:

        
        $$
        \sigma_i = \sqrt{\dfrac{\sum_{j=1}^Tl_{ij}^2-\dfrac{(\sum_{j=1}^Tl_{ij})^2}{T}}{T - 1}}
        $$
        
        И полученные отклонения нормируются:
        
        $$
        \tilde{l}_{ij}=\dfrac{i_{ij}}{\sigma_i}
        $$
        
        3. По нормированным отклонениям вычисляется предварительная средняя сезонная волна:

        $$
        S_{ij}^1=\dfrac{\sum_{i=1}^m\tilde{l}_{ij}}{m}
        $$

        1. Средняя предварительная сезонная волна умножается на среднее квадратическое отклонение каждого года и вычитается из исходного ряда, получается первая оценка тренда:

        $$
        f_{ij}^1=Y_{ij}-S_j^1\sigma_i
        $$

        1. Получаемый тренд сглаживают скользящей средней по пяти точкам и получают новую оценку тренда $f_{ij}^2$. Чтобы не потерять точки в начале и в конце ряда их сглаживают по трем точкам, причем для крайних точек используют специальные формулы сглаживания:
        
        $$
        f_1^2 = \dfrac{5f_1^1+2f_2^1-f_3^1}{6}
        $$
        
        $$
        f_n^2 = \dfrac{5f_n^1+2f_{n-1}^1-f_{n-2}^1}{6}
        $$
        
        2. Вычисляют новые отклонения исходного ряда $y_t$ от тренда $y_t^2$:
        
        $$
        l_t^2=y_t-S_j^2\sigma_i
        $$
        
        для полученных отклонений вновь выполняют пункты 2 и 3 и получают окончательную среднюю сезонную волну $S_j^2$.
        
        3. Вычисляют остаточную компоненту:
        
        $$
        \epsilon_{ij}=l_{ij}^2-S_j^2\sigma_i
        $$
        
        и определяют $k_i$ - коэффициент напряженности сезонной волны)

        $$
        k_i = \dfrac{\sum_{j=1}^Tl_{ij}^2\epsilon_{ij}}{\sum_{j=1}^T\epsilon_{ij}^2}
        $$
    keywords: [66, Метод Четверикова]
num67:
    text: |-
        ## 67) Мультипликативная (аддитивная) модель ряда динамики при наличии тенденции: этапы построения.

        Простейший подход- расчет значений сезонной компоненты методом скользящей средней и построение аддитивной или мультипликативной модели временного ряда. Общий вид аддитивной модели следующий:

        $Y = T + S + E$

        Эта модель предполагает, что каждый уровень временного ряда может быть представлен как произведение трендовой, сезонной и случайной компонент. Общий вид мультипликативной модели выглядит так:

        $Y = T\times S\times E$

        Эта модель предполагает, что каждый уровень временного ряда может быть представлен как произведение трендовой, сезонной и случайной компонент. Выбор одной из двух моделей осуществляется на основе анализа структуры сезонных колебаний. Если амплитуда колебаний приблизительно постоянна, строят аддитивную модель временного ряда, в которой значения сезонной компоненты предполагаются постоянными для различных циклов. Если амплитуда сезонных колебаний возрастает или уменьшается, строят мультипликативную модель временного ряда, которая ставит уровни ряда в зависимость от значений сезонной компоненты.

        Построение аддитивной и мультипликативной моделей сводится к расчету значений трендовой, циклической и случайной компонент для каждого уровня ряда.

        Процесс построения модели включает в себя следующие шаги.

        1. Выравнивание исходного ряда методом скользящей средней.
        2. Расчет значений сезонной компоненты.
        3. Устранение сезонной компоненты из исходных уровней ряда и получение выровненных данных в аддитивной или мультипликативной модели.
        4. Аналитическое выравнивание уровней и расчет значений тренда с использованием полученного уравнения тренда.
        5. Расчет полученных по модели значений или расчет абсолютных и относительных ошибок.

        Если полученные значения ошибок не содержат автокорреляции, ими можно заменить исходные уровни ряда и в дальнейшем использовать временной ряд ошибок для анализа взаимосвязи исходного ряда и других временных рядов.
    keywords: [67, Мультипликативная модель при наличии тенденции, этапы построения]
num68:
    text: |-
        ## 68) Моделирование периодических колебаний (гармоники Фурье).

        Модели временных рядов с периодичными колебаниями строятся на основе использования гармонического анализа.

        Общий вид гармоники Фурье:

        $$
        \begin{equation}
        \hat{y}_t = a + \sum (b_k \cdot \cos(kt) + d_k \cdot \sin(kt)) \quad (11.76)
        \end{equation}
        $$

        где $a, b_k, d_k$— параметры, которые оцениваются с помощью МНК

        $$
        \begin{equation}
        a = \frac{2}{n} \sum y_k, \quad b_k = \frac{2}{n} \sum y_k \cdot \cos(kt), \quad d_k = \frac{2}{n} \sum y_k \cdot \sin(kt)
        \end{equation}
        $$

        В экономических данных эти модели используют для моделирования показателей таких отраслей, как строительство, с/х, туризм и т.д. Обычно $n = 12$.

        На практике строят гармоники первых четырех порядков:

        Первый порядок (k = 1):

        $$
        \begin{equation}
        \hat{y}_{t1} = a + \sum (b_1 \cdot \cos(t) + d_1 \cdot \sin(t))
        \end{equation}
        $$

        Второй порядок (k = 2): 

        $$
        \begin{equation}
        \hat{y}_{t2} = \hat{y}_{t1} + \sum (b_2 \cdot \cos(2t) + d_2 \cdot \sin(2t))
        \end{equation}
        $$

        Третий порядок (k = 3):

        $$
        \begin{equation}
        \hat{y}_{t3} = \hat{y}_{t2} + \sum (b_3 \cdot \cos(3t) + d_3 \cdot \sin(3t))
        \end{equation}
        $$

        Четвертый порядок (k = 4): 

        $$
        \begin{equation}
        \hat{y}_{t4} = \hat{y}_{t3} + \sum (b_4 \cdot \cos(4t) + d_4 \cdot \sin(4t))
        \end{equation}
        $$

        Предпочтение отдается наилучшей гармонике, которая имеет наименьшую среднюю квадратическую ошибку.
    keywords: [68, Моделирование периодических колебаний, гармоники Фурье]
num69:
    text: |-
        ## 69) ⏳Прогнозирование одномерного временного ряда случайной компоненты (распределение Пуассона).

        Прогнозирование одномерного временного ряда случайной компоненты с использованием распределения Пуассона обычно применяется в случаях, когда данные представляют собой дискретные события, происходящие в фиксированные интервалы времени, и эти события редки или их количество невелико. Примеры таких данных включают количество звонков в колл-центр за час, количество аварий на дороге за день, и т.п.

        ### Основные характеристики распределения Пуассона:

        - **Пуассоновский процесс** предполагает, что события происходят с постоянной средней интенсивностью $\lambda$ (среднее количество событий за фиксированный интервал времени).
        - Вероятность наблюдать k событий в фиксированный интервал времени определяется формулой:
            
            $$
            P(X=k) = \dfrac{\lambda^ke^{-\lambda}}{k!}
            $$
            
            где λ — среднее количество событий (интенсивность), 
            k — количество событий, 
            e — основание натурального логарифма.
            

        ### Шаги для прогнозирования с использованием распределения Пуассона:

        1. **Сбор данных:**
        Собрать исторические данные о количестве событий за фиксированные интервалы времени.
        2. **Оценка параметра** $\lambda$**:**
        Оценить параметр $\lambda$ как среднее количество событий за период на основе исторических данных:
            
            $$
            \lambda = \frac{1}{n} \sum_{i=1}^n X_i
            $$
            
            где $X_i$ — количество событий в $i$-ом интервале времени, 
            
            $n$— количество интервалов времени.
            
        3. **Проверка соответствия распределению Пуассона:**
        Проверить, соответствует ли временной ряд пуассоновскому распределению. Это можно сделать с помощью статистических тестов или графических методов, таких как гистограмма или Q-Q график.
        4. **Прогнозирование:**
        Использовать оцененное значение $\lambda$ для прогнозирования будущих значений временного ряда. Прогнозируемое количество событий в следующем интервале времени будет равно $\lambda$.
    keywords: [Прогнозирование одномерного временного ряда, 69]
num70:
    text: |-
        ## 70) Функциональные преобразования переменных в линейной регрессионной модели. Метод Зарембки. Особенности применения.

        Сравниваются две модели:

        $$
        \begin{equation}
        y_i = a_1 + a_2 X_i + u_i
        \end{equation}
        $$

        и

        $$
        \begin{equation}
        \ln y_i = \beta_1 + \beta_2 X_i + \ln u_i
        \end{equation} 
        $$

        Шаг 1. Вычисляется среднее геометрическое по выборке:

        $$
        \begin{equation}
        \bar{Y}_{geom} = \sqrt[n]{y_1 y_2 y_3 \ldots y_n}
        \end{equation}
        $$

        или

        $$
        \begin{equation}
        \bar{Y}_{geom} = \exp \left( \frac{1}{n} \sum_{i=1}^n \ln y_i \right)
        \end{equation}
        $$

        Шаг 2. Наблюдения $y_i$ пересчитываются:

        $$
        \begin{equation}
        y_i^* = \frac{y_i}{\bar{Y}_{geom}}
        \end{equation}
        $$

        Шаг 3. Рассматриваются линейная регрессия с наблюдениями $y_i^*$ *вместо* $y_i$ *и логарифмическая регрессия с наблюдениями* $\ln y_i^*$ вместо $\ln y_i$. В остальном модели не меняются (то есть правая часть остается без изменений):

        $$
        \begin{equation}
        y_i^* = a_1 + a_2 X_i + u_i
        \end{equation} 
        $$

        $$
        \begin{equation}
        \ln y_i^* = \beta_1 + \beta_2 X_i + \ln u_i
        \end{equation}
        $$

        Находим остаточные суммы квадратов остатков для полученных вспомогательных регрессий $ESS_1$ и $ESS_2$.

        Шаг 4. Составляем статистику:

        $$
         \begin{equation}
        Z = \left| \frac{n}{2} \ln \frac{ESS_1}{ESS_2} \right|. \quad (4.57)
        \end{equation}
        $$

        И сравниваем ее с табличным распределением Хи-квадрат с одной степенью свободы. Если $Z > \chi^2_{кр}$, то гипотеза относительно того, что модели (4.56) и (4.57) не имеют статистически значимых различий, отвергается, то есть выбираем модель полулогарифмическую, если $Z \leq \chi^2_{кр}$ — выбираем линейную модель.
    keywords: [70, Метод Зарембки]
num71:
    text: |-
        ## 71) Функциональные преобразования переменных в линейной регрессионной модели. Тест Бокса-Кокса. Особенности применения.

        Тест Бокса-Кокса основывается на утверждении о том, что $(y - 1)$ и $\ln y$ являются частными случаями функции вида:

        $$
        \begin{equation}
        F = \frac{y^\lambda - 1}{\lambda}.
        \end{equation}
        $$

        Если параметр $\lambda = 1$, то данная функция принимает вид $F = y - 1$.

        Если параметр $\lambda \to 0$, то функция принимает вид $F = \ln y$.

        Для того чтобы определить оптимальное значение параметра $\lambda$, необходимо провести несколько серий экспериментов с множеством значений данного параметра. С помощью такого перебора можно рассчитать такое значение параметра $\lambda$, которое даст минимальную величину критерия суммы квадратов отклонений. Подобный метод вычисления оптимального значения параметра называется поиском на решётке или на сетке значений.

        Шаг 1. Преобразуем зависимую переменную по методу П. Зарембы:

        $$
        \begin{equation}
        y_i^* = \frac{y_i}{\sqrt[n]{y_1 y_2 y_3 \ldots y_n}} = \frac{y_i}{\exp \left( \frac{1}{n} \sum_{i=1}^n \ln y_i \right)}.
        \end{equation}
        $$

        Шаг 2. Рассчитываются новые переменные (преобразование Бокса-Кокса):

        $$
        \begin{equation}
        y_{i(B-C)} = \frac{y_i^\lambda - 1}{\lambda}, \quad X_{i(B-C)} = \frac{X_i^\lambda - 1}{\lambda}. \quad (4.58)
        \end{equation}
        $$

        Шаг 3. Оценивается линейная модель регрессии с использованием масштабированных значений при $\lambda$ от 0 до 1:

        $$
        \begin{equation}
        y_{i(B-C)} = a_1 + a_2 X_{i(B-C)} + u_i. \quad (4.59)
        \end{equation}
        $$

        Шаг 4. Выбирается минимальное значение суммы квадратов остатков ESS, выбирается одна из крайних регрессий, к которой ближе точка минимума.

        Результативная переменная y в нормальной линейной модели регрессии является непрерывной величиной, способной принимать любые значения из заданного множества. Но помимо нормальных линейных моделей регрессии существуют модели регрессии, в которых переменная y должна принимать определённый узкий круг заранее заданных значений.
    keywords: [71, Тест Бокса-Кокса]
num72:
    text: |-
        ## 72) Функциональные преобразования переменных в линейной регрессионной модели. Критерий Акаике и Шварца. Особенности применения.

        ### Акаике

        Учитывает как точность, так и сложность модели. Он штрафует модели за использование большего количества параметров, что помогает избежать переобучения модели.

        1. Определите модели, которые вы хотите сравнить.
        2. Оцените параметры каждой модели с помощью метода максимального правдоподобия.
        3. Вычислите значение AIC для каждой модели:
            
            $$
            \begin{equation}
            AIC = \ln \left( \frac{ESS_m}{n} \right) + \frac{2m}{n} + 1 + \ln (2\pi).
            \end{equation}
            $$
            
            При увеличении объясняющих переменных первое слагаемое в правой части уменьшается, второе — увеличивается.
            
        4. Выберите модель с наименьшим значением критерия AIC.
            
            Среди нескольких альтернативных моделей (полной и редуцированной) предпочтение отдаётся модели, с наименьшим значением статистики AIC, в которой достигается определённый компромисс между величиной остаточной суммы квадратов и количеством объясняющих переменных.
            
            Условия применения критерия AIC:
            
            - Сравниваемые модели построены на одном и том же наборе данных.
            - Сравниваемые модели и имеют одну и ту же объясняемую переменную.
            - Обучающая выборка имеет бесконечный размер.

        ### Шварц

        Байесовский информационный критерий также учитывает точность и сложность модели, но в отличие от AIC, он штрафует модели за использование большего количества параметров сильнее. Это помогает избежать переобучения модели и выбрать более простую модель, которая всё ещё точно описывает данные.

        $$
        \begin{equation}
        SC = \ln \left( \frac{ESS_m}{n} \right) + \frac{m \ln(n)}{n} + 1 + \ln(2\pi).
        \end{equation}
        $$

        При увеличении количества объясняющих переменных первое слагаемое в правой части уменьшается, а второе — увеличивается. Среди нескольких альтернативных моделей (полной и редуцированной) предпочтение отдаётся модели с наименьшим значением статистики Шварца.

        Байесовский информационный критерий (критерий Шварца) является аналогом критерия Акаике с более строгой функцией штрафа, которая также зависит от размерности модели.
    keywords: [72, Критерий, Акаике, Шварца]
num73:
    text: |-
        ## 73)Функциональные преобразования переменных в линейной регрессионной модели. Тест МакАлера. Особенности применения.

        Функциональное преобразование переменных в линейной регрессионной модели является одной из задач выбора спецификации модели. Есть различные виды преобразований, такие как логарифмическое, полулогарифмическое, квадратичное, экспоненциальное и т.д. Все они сводятся к оценке параметров линейной регрессионной модели.

        Тест МакАлера — тест для выбора спецификации модели между полулогарифмической и линейной модель. Суть его использования заключается в том, что нельзя выбрать между линейной и полулогарифмической моделью по  $R^2$, поэтому следует применять тесты для выбора.

        Тест МакАлера состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat{\ln y}$
            1. Составим вспомогательные регрессии: 
            
            $$
            y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
            $$
            
            $$
            \ln y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_2
            $$
            
        2. Составим вспомогательные регресии

        $$
        e^{\widehat{\ln y}} = \beta_0 + \sum_{i=1}^k\beta_i x_i + e_{1,i}
        $$

        $$
        \ln \hat y = \beta_0 + \sum_{i=1}^k\beta_i x_i + e_{2,i}
        $$

        Получив данные регрессии, вычислим остатки  $\hat e_{1,i}$  и  $\hat e_{2, i}$, которые в дальнейшем будем использовать как регрессоры

        1. Оценим новые регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \theta_1 \hat e_{1, i} + u_i
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \theta_2 \hat e_{2, i} + u_i
        $$

        1. Найденные значения $\hat \theta_1$ и $\hat \theta_2$ проверяем на значимость:
            1. Если  $\hat \theta_1$ значим и $\hat \theta_2$ не значим, выбираем полулогарифмическую модель
            2. Если  $\hat \theta_2$ значим и $\hat \theta_1$ не значим значит выбираем линейную модель
            3. Если  $\hat \theta_1$ и   $\hat \theta_2$ оба значимы или не значимы, значит тест не позволяет нам выбрать одну из моделей.
    keywords: [73, Тест МакАлера]
num74:
    text: |-
        ## 74) Функциональные преобразования переменных в линейной регрессионной модели. Тест МакКиннона. Особенности применения.

        Функциональное преобразование переменных в линейной регрессионной модели является одной из задач выбора спецификации модели. Есть различные виды преобразований, такие как логарифмическое, полулогарифмическое, квадратичное, экспоненциальное и т.д. Все они сводятся к оценке параметров линейной регрессионной модели.

        Тест МакКиннона (PE-тест) — тест для выбора между логарифмической и линейной или полулогарифмической спецификаций моделей.

        Если выбор стоит между **логарифмической** и линейной моделью, тест МакКиннона состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat {\ln y}$
            
            $$
            y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
            $$
            

        $$
        \ln y = \beta_0 + \sum_{i=1}^k\beta_i \ln x_i + \epsilon_2
        $$

        1. Оценим вспомогательные регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_1 ( \hat y - e^{\widehat{\ln y}}) + e_1
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_2 ( \ln \hat y - {\widehat{\ln y}}) + e_2
        $$

        1.   Сравниваем значимость δ:
            1. Если $\delta_1$ значим, а $\delta_2$ не значим, выбираем логарифмическую модель
            2. Если $\delta_2$ значим, а $\delta_1$ не значим, выбираем линейную модель
            3. Если $\delta_1$ и $\delta_2$ оба значимы или не значимы, то выбор модели невозможен.

        Если выбор стоит между **полулогарифмической** и линейной моделью, тест МакКиннона состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat {\ln y}$

        $$
        y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
        $$

        $$
        \ln y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_2
        $$

        1. Оценим вспомогательные регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_1 ( \hat y - e^{\widehat{\ln y}}) + e_1
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_2 ( \ln \hat y - {\widehat{\ln y}}) + e_2
        $$

        1. Сравниваем значимость δ:
            1. Если $\delta_1$ значим, а $\delta_2$ не значим, выбираем логарифмическую модель
            2. Если $\delta_2$ значим, а $\delta_1$ не значим, выбираем линейную модель
            3. Если $\delta_1$ и $\delta_2$ оба значимы или не значимы, то выбор модели невозможен.
    keywords: [74, Тест МакКиннона]
num75:
    text: |-
        ## 75) Функциональные преобразования переменных в линейной регрессионной модели. Тест Уайта. Особенности применения.

        Функциональное преобразование переменных в линейной регрессионной модели является одной из задач выбора спецификации модели. Есть различные виды преобразований, такие как логарифмическое, полулогарифмическое, квадратичное, экспоненциальное и т.д. Все они сводятся к оценке параметров линейной регрессионной модели.

        Тест Уайта (PE-тест) — тест для выбора между логарифмической и линейной или полулогарифмической спецификаций моделей.

        Если выбор стоит между **логарифмической** и линейной моделью, тест Уайта состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat {\ln y}$
            
            $$
            y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
            $$
            

        $$
        \ln y = \beta_0 + \sum_{i=1}^k\beta_i \ln x_i + \epsilon_2
        $$

        1. Оценим вспомогательные регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_1 ( \hat y - e^{\widehat{\ln y}}) + e_1
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_2 ( \ln \hat y - {\widehat{\ln y}}) + e_2
        $$

        1.   Сравниваем значимость δ:
            1. Если $\delta_1$ значим, а $\delta_2$ не значим, выбираем логарифмическую модель
            2. Если $\delta_2$ значим, а $\delta_1$ не значим, выбираем линейную модель
            3. Если $\delta_1$ и $\delta_2$ оба значимы или не значимы, то выбор модели невозможен.

        Если выбор стоит между **полулогарифмической** и линейной моделью, тест Уайта состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat {\ln y}$

        $$
        y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
        $$

        $$
        \ln y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_2
        $$

        1. Оценим вспомогательные регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_1 ( \hat y - e^{\widehat{\ln y}}) + e_1
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_2 ( \ln \hat y - {\widehat{\ln y}}) + e_2
        $$

        1. Сравниваем значимость δ:
            1. Если $\delta_1$ значим, а $\delta_2$ не значим, выбираем логарифмическую модель
            2. Если $\delta_2$ значим, а $\delta_1$ не значим, выбираем линейную модель
            3. Если $\delta_1$ и $\delta_2$ оба значимы или не значимы, то выбор модели невозможен.
    keywords: [75, Тест Уайта]
num76:
    text: |-
        ## 76) Функциональные преобразования переменных в линейной регрессионной модели. Тест Дэвидсона. Особенности применения.

        Функциональное преобразование переменных в линейной регрессионной модели является одной из задач выбора спецификации модели. Есть различные виды преобразований, такие как логарифмическое, полулогарифмическое, квадратичное, экспоненциальное и т.д. Все они сводятся к оценке параметров линейной регрессионной модели.

        Тест Дэвидсона (PE-тест) — тест для выбора между логарифмической и линейной или полулогарифмической спецификаций моделей.

        Если выбор стоит между **логарифмической** и линейной моделью, тест Дэвидсона состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat {\ln y}$
            
            $$
            y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
            $$
            

        $$
        \ln y = \beta_0 + \sum_{i=1}^k\beta_i \ln x_i + \epsilon_2
        $$

        1. Оценим вспомогательные регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_1 ( \hat y - e^{\widehat{\ln y}}) + e_1
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_2 ( \ln \hat y - {\widehat{\ln y}}) + e_2
        $$

        1.   Сравниваем значимость δ:
            1. Если $\delta_1$ значим, а $\delta_2$ не значим, выбираем логарифмическую модель
            2. Если $\delta_2$ значим, а $\delta_1$ не значим, выбираем линейную модель
            3. Если $\delta_1$ и $\delta_2$ оба значимы или не значимы, то выбор модели невозможен.

        Если выбор стоит между **полулогарифмической** и линейной моделью, тест Дэвидсона состоит из следующих шагов:

        1. Найдем оцененные значения зависимой переменной в каждой модели $\hat y$, $\widehat {\ln y}$

        $$
        y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_1
        $$

        $$
        \ln y = \beta_0 + \sum_{i=1}^k\beta_i x_i + \epsilon_2
        $$

        1. Оценим вспомогательные регрессии:

        $$
        \ln y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_1 ( \hat y - e^{\widehat{\ln y}}) + e_1
        $$

        $$
        y = \beta_0 + \sum_{i = 1}^k \beta_i x_i + \delta_2 ( \ln \hat y - {\widehat{\ln y}}) + e_2
        $$

        1. Сравниваем значимость δ:
            1. Если $\delta_1$ значим, а $\delta_2$ не значим, выбираем логарифмическую модель
            2. Если $\delta_2$ значим, а $\delta_1$ не значим, выбираем линейную модель
            3. Если $\delta_1$ и $\delta_2$ оба значимы или не значимы, то выбор модели невозможен.
    keywords: [76, Тест Дэвидсона]
num77:
    text: |-
        ## 77) Модели с распределенными лаговыми переменными.

        Модели с лаговыми переменными — это модели содержащие в качестве объясняющих переменных лаговые переменные. Лаговые переменные — это переменные взятые в модели регрессии с запаздыванием во времени. Величина запаздывания называется лагом.

        Существуют несколько видов моделей с лаговыми переменными, такие как:

        1.  Модели с распределенными лагами, где содержатся лаги объясняющих переменных

        $$
        y_t = a + b_0x_t + b_1x_{t-1} + ... + b_k x_{t-k} + \epsilon_t
        $$

        1. Модели авторегрессии, где содержатся лаги зависимых переменных

        $$
        y_t = a + b_0x_t + c_1y_{t-1} + ... c_k y_{t-k} + \epsilon_t
        $$

        1. Смешанные модели, где содержатся лаги как зависимых, так и независимых переменных

        $$
        y_t = a + b_1y_{t-1} + ... + b_ky_{t-k} + c_0x_t + c_1 x_{t-1} + ... + c_k x_{t-k} + \epsilon_t
        $$

        К причинам наличия лагов в экономических данных относят:

        1. Психологические причины, связанные с поведением людей (покупателей)
        2. Технологические причины. В условиях быстрого развития технологий использование новейших инноваций невозможно, поэтому происходит задержка, обусловленная сроком службы старого оборудования/услуг
        3. Институциональные причины. Вызваны в основном сроком действия контрактов
        4. Механизм формирования экономических показателей.
    keywords: [77, распределенными лаговыми переменными, Модели, лаговыми]
num78:
    text: |-
        ## 78) Оценка моделей с лагами в независимых переменных. Преобразование Койка.

        Модели с лагами в независимых переменных — это модели, которые имеют в своей спецификации значения объясняющих переменных с лагом — некоторым запаздыванием. Данные модели подразделяются на два вида:

        1. Модели с конечным числом лагов, Спецификация:

        $$
        y_t = a_0 + b_0x_t + b_1x_{t-1} + ...+ b_k x_{t-k} + \epsilon_t = a_0 + \sum_{i=0}^k b_i x_{t-i} + \epsilon_t
        $$

        $b_0$ — краткосрочный мультипликатор, показывает изменение среднего значения $y_t$ под воздействием переменной $x_t$ на единицу

        $\sum_{i=0}^k b_i$ — долгосрочный мультипликатор, который показывает изменение среднего значения $y_t$ под воздействием единичного изменения переменной $x_t$ в каждом из исследуемых периодов

        Параметры оцениваются с помощью МНК и через замену лаговых переменных:

        $$
        y_t = a_0 +b_0z_{0t} + b_1z_{1t} + ... b_kz_{kt} + \epsilon_t
        $$

        $$
        z_{0t} = x_t, z_{1t} = x_{t-1},..., z_{kt} = x_{t-k}, \qquad t = 1, ..., n
        $$

        1. Модели с бесконечным числом лагов, Спецификация:

        $$
        y_t = a_0 + b_0x_t + b_1x_{t-1} + ...+ \epsilon_t = a_0 + \sum_{i=0}^\infty b_i x_{t-i} + \epsilon_t
        $$

        Для оценки параметров этой модели используются методы геометрической прогрессии последовательного увеличения числа лагов, и метод Койка.

        Метод Койка основан на естественном предположении о том, что степень влияния лаговой переменной убывает по мере возрастания лага. Причем такое убывание происходит согласно закону убывающей геометрической прогрессии, и коэффициенты лагов соответственно равны $b_0$, $b_0 \lambda$, $b_0 \lambda^2$, …

        Используя такое представление коэффициентов, спецификация модели принимает следующий вид:

        $$
        y_t = a_0 + b_0 x_t + b_0\lambda x_{t-1} + b_0\lambda^2x_{t-2} + ... + \epsilon_t
        $$

        Количество неизвестных коэффициентов сократилось до 3. Для представления спецификации модели с конечным количеством объясняющих переменных воспользуемся Преобразованием Койка.

        Запишем это же уравнение, только для момента времени $t-1$ и умножим его на $\lambda$

        $$
        \lambda y_{t-1} = \lambda a_0 + b_0 \lambda x_{t-1} + b_0\lambda^2 x_{t-2} + b_0\lambda^3x_{t-3} + ... + \epsilon_t
        $$

        Теперь вычтем из первого уравнения второе, и сократив и сгруппировав все переменные получим следующее:

        $$
        y_t = (1- \lambda)a_0 + b_0 x_t + \lambda y_{t-1} + (\epsilon_t - \lambda \epsilon_{t-1})
        $$

        Данное уравнения является результатом преобразования Койка и является переходом от модели с бесконечным числом лаговых независимых переменным в авторегрессионное с одним лагом зависимой переменной.
    keywords: [78, Преобразование Койка]
num79:
    text: |-
        ## 79) Полиномиально распределенные лаги Алмон.

        Модель Алмона — модель с лаговыми переменными, которая предполагает, что значения коэффициентов $b_j$ могут быть аппроксимированы полиномами соответствующей степени от величины лага j:

        $$
        \beta_i = \alpha_0 + \alpha_1 i + \alpha_2 i^2 +... + \alpha_mi^m
        $$

        Предполагая, что $\beta_i$ имеет квадратичную зависимость, тогда спецификация может быть представлена в виде

        $$
        y_t = \alpha + \sum_{i=0}^k(\alpha_0 + \alpha_1i+ \alpha_2i^2)x_{t-i} + \epsilon_t = \\ \alpha + \alpha_0 \sum_{i=0}^k x_{t-i} + \alpha_1 \sum_{i=0}^k i  x_{t-i} + \alpha_2\sum_{i=0}^ki^2x_{t-i}
        $$

        Если ввести новые переменные 

        $$
        z_{t0} = \alpha_0 \sum_{i=0}^kx_{t-1}, \quad z_{t1} = \alpha_1 \sum_{i=0}^kix_{t-1}, \quad z_{t2} = \alpha_2 \sum_{i=0}^ki^2x_{t-1}
        $$

        И заменить ими выражения сумм, то модель примет следующий вид:

        $$
        y_t = \alpha + a_0 z_{t0} + a_1z_{t1} + a_2z_{t2}
        $$

        Коэффициенты модели можно оценить с помощью МНК и при этом случайные отклонения удовлетворяют предпосылкам МНК.
    keywords: [79, лаги Алмон, Алмон]
num80:
    text: |-
        ## 80) Авторегрессионные модели

        Авторегрессионная модель — это модель, имеющая в качестве объясняющих переменных зависимые переменные с некоторым лагом, в общем случае, модель имеет следующий вид:

        $$
        y_t = a + b_0x_t + c_1y_{t-1} + ... c_k y_{t-k} + \epsilon_t
        $$

        Также выделяют отдельные виды авторегрессионных моделей, например:
        1. Модель адаптивных ожиданий

        $$
        Y_t = a + bX_{t+1}^* + \epsilon_t
        $$

        В данной модели в уравнение регрессии в качестве объясняющей переменной вместо текущего значения $X_t$ входит ожидаемое долгосрочное значение $X_{t+1}^*$

        Выдвигается предположение, что ожидаемое значение связано с текущим, из чего следует следующая замена:

        $$
        X_{t+1}^* = \gamma \cdot X_t + (1- \gamma)\cdot X_t^*
        $$

        $$
        X_{t+1}^* - X_t^*= \gamma \cdot  (X_t - X_t^*), \qquad 0 \leq \gamma \leq 1
        $$

        Заменим ожидаемое значение в исходной спецификации и получим следующий вид:

        $$
        Y_{t-1} = a + b[\gamma \cdot X_{t-1} + (1 - \gamma)X_{t-1}^*] + \epsilon_{t-1}  
        $$

        Используя преобразование Койка, запишем предыдущее уравнение для момента времени $t-1$ и умножим его на $(1-\gamma)$:

        $$
        (1-\gamma)Y_{t-1} = (1-\gamma)a + (1-\gamma)b[\gamma \cdot X_{t-1} + (1 - \gamma)X_{t-1}^*] + (1-\gamma)\epsilon_{t-1}  
        $$

        Исходя из разности этих двух уравнений можно получить следующую спецификацию:

        $$
        Y_t = \gamma a + \gamma b X_t + (1-\gamma)\cdot Y_{t-1} = \alpha + \beta X_t + \mu Y_{t-1} + u_t
        $$

        1. Модель частичной корректировки, Спецификация:

        $$
        Y_t^* = a + b X_t + \epsilon_t
        $$

        Где $Y_t^*$ - желаемое значение $Y_t$ . В данной модели мы предсказываем не фактические, а желаемые значения целевой переменной. Так как значения $Y_t^*$ не являются реально существующими, относительно него выдвигается предположение частичной корректировки

        $$
        Y_t - Y_{t-1} = \lambda (Y_t^* - Y_{t-1})
        $$

        Где $\lambda$ —  коэффициент корректировки. Исходя из этого выражения можно выразить $Y_t$  и затем подставить  вместо $Y_t^*$ соответствующее равенство:

        $$
        Y_t = \lambda a + (1-\lambda)Y_{t-1} + \lambda bX_t +  \lambda \epsilon_t
        $$
    keywords: [80, Авторегрессионные модели]
num81:
    text: |-
        ## 81) Авторегрессионные модели с распределенными лагами.

        **Модель авторегрессии и распределённого лага —** 

        модель временного ряда, в которой текущие значения ряда зависят как от прошлых значений этого ряда, так и от текущих и прошлых значений других временных рядов. Модель ADL(p, q) с одной экзогенной переменной имеет вид:

        $$
        y_t = a_0 + \sum_{i=1}^p a_iy_{t-i} + \sum_{j=0}^q b_j x_{t-j} + \epsilon_t
        $$

        Модель ADL(p, 0) — это модель авторегрессии AR(p), а модель ADL(0, q) — это модель распределенного лага DL(q).

        Также в случае нескольких экзогенных переменных модель может обозначаться как ADL(p, q1, q2, … , qk), где p — это количество зависимых переменных с лагом, q — количество независимых переменных с лагом, а k — количество независимых переменных. Также существуют отдельные случаи, когда для каждой переменной выбирается отдельное количество лагов.
    keywords: [81, Авторегрессионные модели, распределенными лагами]
num82:
    text: |-
        ## 82) Стационарные временные ряды. Определения стационарности, лаговой переменной, автоковариационной функции временного ряда, автокоррляционной функции, коррелограммы, коэффициенты корреляции между разными элементами стационарного временного ряда с временным лагом h.

        - **Стационарность:**
        - Ряд t y называется стационарным в узком смысле, если совместное распределение m наблюдений y_t1 , y_t2 ,...y_tm не зависит от сдвига по времени
        - Временной ряд считается стационарным в широком смысле, если его статистические свойства (среднее, дисперсия, автоковариационная функция) не меняются со временем. Другими словами, ряд не имеет тренда и сезонности.
            
            $$
            E(y_t) = E(y_{t+\tau}) = \mu  \\
            Var(y_t) = Var(y_{t+\tau}) = \gamma(0) \\ 
            
            Cov(y_t, y_{t+\tau}) = \gamma(\tau)
            $$
            
        - **Лаговая переменная:** Это значение временного ряда в предыдущий момент времени. Например, Yt-1 - это лаговая переменная для Yt.
        - **Автоковариационная функция временного ряда:** Измеряет взаимосвязь между значениями временного ряда с определенным временным лагом.
            
            $$
            \gamma(h) = \text{Cov}(X_t, X_{t+h}) = \mathbb{E}[(X_t - \mu)(X_{t+h} - \mu)]
            $$
            
        - **Автокорреляционная функция ACF:** Это нормированная автоковариационная функция, показывающая степень линейной зависимости между значениями временного ряда с заданным лагом.

        $$

        \rho(h) = \frac{\gamma(h)}{\gamma(0)} = \frac{\text{Cov}(X_t, X_{t+h})}{\text{Var}(X_t)} = \frac{\mathbb{E}[(X_t - \mu)(X_{t+h} - \mu)]}{\mathbb{E}[(X_t - \mu)^2]}

        $$

        - **Коррелограмма:** Графическое представление автокорреляционной функции, показывающее как автокорреляция изменяется в зависимости от лага. Она показывает автокорреляции для различных лагов и используется для выявления повторяющихся паттернов в данных, таких как сезонность.
        - **Коэффициенты корреляции между разными элементами стационарного временного ряда с временным лагом h:** Коэффициенты корреляции между разными элементами стационарного временного ряда с временным лагом h. То есть значение автокорреляционной функции для данного лага h. обозначается как ρ(h)
    keywords: [82, Стационарные временные ряды, Определения]
num83:
    text: |-
        ## 83) Стационарные временные ряды. Определения частной автокорреляционной функции, белого шума, автоковариационная функция для белого шума, ACF для белого шума, частная автокорреляционная функция для белого шума.

        - **Стационарность:**
        - Ряд t y называется стационарным в узком смысле, если совместное распределение m наблюдений y_t1 , y_t2 ,...y_tm не зависит от сдвига по времени
        - Временной ряд считается стационарным в широком смысле, если его статистические свойства (среднее, дисперсия, автоковариационная функция) не меняются со временем. Другими словами, ряд не имеет тренда и сезонности.

        $$
        E(y_t) = E(y_{t+\tau}) = \mu  \\
        Var(y_t) = Var(y_{t+\tau}) = \gamma(0) \\ 

        Cov(y_t, y_{t+\tau}) = \gamma(\tau)
        $$

        - **Частная автокорреляционная функция (PACF):** Измеряет корреляцию между значениями временного ряда с лагом h, исключая влияние промежуточных лагов.
        - **Белый шум:** Это случайный временной ряд, значения которого независимы и имеют одинаковое распределение.
            
            $$
            E(εt) = 0 \\
            Var(εt) = σ^2 \\
            Cov(εt, εt-h) = 0, h ≠ 0 \\
            $$
            
        - **Автоковариационная функция для белого шума:**

        $$
        γ(0) = σ^2, γ(h) = 0, h ≠ 0
        $$

        $$
        ρ(0) = 1, ρ(h) = 0, h ≠ 0
        $$

        - **Частная автокорреляционная функция для белого шума:**

        $$
        φ(0) = 1, φ(h) = 0, h ≠ 0
        $$
    keywords: [83, Стационарные временные ряды, Стационарные, автокорреляционной функции]
num84:
    text: |-
        ## 84) Модели стационарных временных рядов: модель ARMA(p, q) (классический вид и через лаговый оператор). Авторегрессионный многочлен, авторегрессионная часть и часть скользящего среднего.

        - **Стационарность:**
        - Ряд t y называется стационарным в узком смысле, если совместное распределение m наблюдений y_t1 , y_t2 ,...y_tm не зависит от сдвига по времени
        - Временной ряд считается стационарным в широком смысле, если его статистические свойства (среднее, дисперсия, автоковариационная функция) не меняются со временем. Другими словами, ряд не имеет тренда и сезонности.

        $$
        E(y_t) = E(y_{t+\tau}) = \mu  \\
        Var(y_t) = Var(y_{t+\tau}) = \gamma(0) \\ 

        Cov(y_t, y_{t+\tau}) = \gamma(\tau)
        $$

        - **Модель ARMA(p, q):** Это модель авторегрессионного скользящего среднего, описывающая временной ряд как линейную комбинацию его собственных прошлых значений (AR часть) и случайных ошибок (MA часть).
            - **Классический вид:**
                
                $$
                Y_t = \gamma + a_1 Y_{t-1} + a_2 Y_{t-2} + \cdots + a_p Y_{t-p} + b_1 \epsilon_{t-1} + b_2 \epsilon_{t-2} + \cdots + b_q \epsilon_{t-q}
                
                $$
                
            - **Через лаговый оператор (L):Авторегрессионный многочлен:** Это выражение (1 - a1L - a2L^2 - ... - apL^p).
                
                $$
                (1 - a_1 L - a_2 L^2 - \cdots - a_p L^p) Y_t = (1 + b_1 L + b_2 L^2 + \cdots + b_q L^q) \epsilon_t
                
                $$
                
                    Сама модель:
                
            
            $$
            L^k Y_t = Y_{t-k}
            $$
            

        **Авторегрессионная часть AR**: Левая часть уравнения представляет собой авторегрессионную часть модели. Это часть модели, которая включает лаговые значения временного ряда, где каждое $L^i Y_t$ означает $Y_{t-i}$, то есть значение временного ряда на $i$-й лаг

        **Часть скользящего среднего MA**:  Правая часть уравнения, это часть модели, которая включает лаговые значения случайных ошибок. представляет собой часть скользящего среднего, где каждое $L^i \varepsilon_t$ означает $\varepsilon_{t-i}$, то есть значение случайной ошибки на $i$-й лаг.
    keywords: [84, модель ARMA, Авторегрессионный многочлен]
num85:
    text: |-
        ## 85) Модели стационарных временных рядов: модель ARMA(1, q). Доказательство утверждения: Модель ARMA(1, q) стационарна тогда и только тогда, когда|a| < 1.

        - **Стационарность:**
        - Ряд t y называется стационарным в узком смысле, если совместное распределение m наблюдений y_t1 , y_t2 ,...y_tm не зависит от сдвига по времени
        - Временной ряд считается стационарным в широком смысле, если его статистические свойства (среднее, дисперсия, автоковариационная функция) не меняются со временем. Другими словами, ряд не имеет тренда и сезонности.

        $$
        E(y_t) = E(y_{t+\tau}) = \mu  \\
        Var(y_t) = Var(y_{t+\tau}) = \gamma(0) \\ 

        Cov(y_t, y_{t+\tau}) = \gamma(\tau)
        $$

        - **Модель ARMA(p, q):** Это модель авторегрессионного скользящего среднего, описывающая временной ряд как линейную комбинацию его собственных прошлых значений (AR часть) и случайных ошибок (MA часть).

        **ARMA(1 q):** 

        $$
        Y_t = a_1 Y_{t-1} + \epsilon_t + b_1 \epsilon_{t-1} + b_2 \epsilon_{t-2} + \cdots + b_q \epsilon_{t-q}
        $$

        - Доказательство утверждания:
        - **ACF и стационарность:** Стационарный ряд, по определению, не имеет тренда и сезонности. Это означает, что взаимосвязь между значениями ряда, разделенных большим временным промежутком, слаба или отсутствует. ACF ((автокорреляционная функция) отражает зависимость между значениями временного ряда с определенным временным лагом), отражая эту слабую зависимость, быстро убывает к нулю при увеличении лага.

        **1. Необходимость:**

        Докажем, что если |a1| ≥ 1, то модель ARMA(1, q) нестационарна.

        - Предположим, что |a1| ≥ 1.
        - Рассмотрим автокорреляционную функцию (ACF) модели ARMA(1, q): ρ(h).
        - Из формулы для ACF модели ARMA(1, q) следует, что ρ(h) = a1^h.
        - Поскольку |a1| ≥ 1, то |a1|^h не стремится к нулю при h → ∞.
        - Следовательно, ACF не убывает с ростом лага, что является признаком нестационарности.

        **2. Достаточность:**

        Докажем, что если |a1| < 1, то модель ARMA(1, q) стационарна.

        - Предположим, что |a1| < 1.
        - Как показано выше, ACF модели ARMA(1, q) ρ(h) = a1^h.
        - Поскольку |a1| < 1, то |a1|^h стремится к нулю при h → ∞.
        - Следовательно, ACF убывает с ростом лага, что является признаком стационарности.
        - Кроме того, среднее значение модели ARMA(1, q) остается постоянным и равным c/(1-a1), а дисперсия также не меняется со временем.
    keywords: [85, ARMA]
num86:
    text: |-
        ## 86) Модели стационарных временных рядов: Модель MA(q), Среднее, дисперсия и ACF для MA(q). Модель MA(∞).

        - **Модель MA(q):** Это модель скользящего среднего, описывающая временной ряд как линейную комбинацию его собственных прошлых случайных ошибок.**/text{Среднее:** } E(Y_t) = c

        $$
        Y_t = c + b_1ε_{t-1} + b_2ε_{t-2} + ... + b_qε_{t-q} + ε_t
        $$

        $$
        \text{Среднее: } E(Y_t) = c
        $$

        $$
        \text{Дисперсия: } Var(Y_t) = σ^2(1 + b_1^2 + b_2^2 + ... + b_q^2)
        $$

        - **ACF для MA(q):** ρ(h) = 0 для h > q.
        - **Модель MA(∞):** Это обобщение MA(q), где случайная ошибка может зависеть от бесконечного числа предыдущих ошибок.
    keywords: [86, Модель MA]
num87:
    text: |-
        ## 87) Модели стационарных временных рядов: Модель AR(p). Доказательство утверждения: Модель AR(p) определяет стационарный ряд ⇐⇒ выполнено условие стационарности: все корни многочлена az по модулю больше единицы. Модель AR(1).

        - **Модель AR(p):** Это модель авторегрессии, описывающая временной ряд как линейную комбинацию его собственных прошлых значений.
            
            $$
            Y_t = c + a_1Y_{t-1} + a_2Y_{t-2} + ... + a_pY_{t-p} + ε_t
            $$
            
        - **Условие стационарности:** Все корни многочлена az по модулю больше единицы.
        - **Модель AR(1):**
            
            $$
            Y_t = c + a_1Y_{t-1} + ε_t
            $$
            

        **Доказательство:**

        Для модели AR(p) определен характеристический многочлен $a(z)$. Пусть $z_1, ..., z_p$ - все его корни. Тогда многочлен примет вид:
        $$
        a(z) = (z - z_1)...(z - z_p)
        $$
        Или, вынеся корни за скобку:
        $$
        a(z) = z_1...z_p(1 - \frac{z}{z_1})...(1 - \frac{z}{z_p})
        $$
        Имеем модель AR следующего вида:
        $$
        a(L)y_t = \mu + \epsilon_t
        $$
        Выразим временной ряд:
        $$
        y_t = \frac{\mu + \epsilon_t}{a(L)}
        $$
        При подстановке многочлена и дальнейшем разложении правой части на простые дроби непременно возникнут слагаемые следующего вида:
        $$
        \frac{\epsilon_t}{1 - \frac{z}{z_i}}
        $$
        Если корень лежит внутри единичной окружности, то есть $|z_i| \leq 1$, то соответствующий ряд будет расходиться, а значит не обладать стационарностью. Следовательно, при всех корнях $|z_i| > 1$ ряд будет стационарным, ч.т.д.
    keywords: [87, Модель AR]
num88:
    text: |-
        ## 88) Прогнозирование для модели ARMA. Условия прогнозирования. Периоды прогнозирования. Информативность прогнозов.

        Прогнозирование временных рядов с использованием модели ARMA (AutoRegressive Moving Average) включает в себя предсказание будущих значений на основе уже имеющихся данных. Прогнозирование осуществляется путем экстраполяции временного ряда с использованием авторегрессионных и скользящих средних компонентов модели.

        ### Условия прогнозирования

        1. **Стационарность**:
            - Модель ARMA предполагает, что временной ряд стационарен. Стационарность означает, что временной ряд имеет постоянное среднее, постоянную дисперсию и постоянную автокорреляционную структуру во времени.
            - Если временной ряд не стационарен, необходимо сделать его стационарным путем дифференцирования или других преобразований перед применением модели ARMA.
        2. **Корректная спецификация модели**:
            - Выбор правильных значений параметров p и q для модели ARMA(p, q) важен для точного прогнозирования. Эти параметры определяют порядок авторегрессионного и скользящего среднего компонентов.
                
                pp
                
                qq
                
            - Обычно параметры модели выбираются с использованием критериев информации, таких как AIC (Akaike Information Criterion) или BIC (Bayesian Information Criterion).
        3. **Оценка остатков**:
            - Остатки модели (разница между наблюдаемыми и предсказанными значениями) должны быть белым шумом, что указывает на адекватность модели. Остатки должны быть независимыми и одинаково распределенными с нулевым средним.

        ### Периоды прогнозирования

        Прогнозирование с использованием модели ARMA можно разделить на два типа в зависимости от периода прогнозирования:

        1. **Краткосрочное прогнозирование**:
            - Прогнозы на несколько шагов вперед (обычно до 10 шагов). Такие прогнозы обычно более точны, так как они основаны на последних наблюдениях и имеют меньшую неопределенность.
            - Пример: прогнозирование продаж на следующий месяц или квартал.
        2. **Долгосрочное прогнозирование**:
            - Прогнозы на много шагов вперед (более 10 шагов). Такие прогнозы менее точны из-за накопления ошибок и неопределенности.
            - Пример: прогнозирование экономических показателей на несколько лет вперед.

        ### Информативность прогнозов

        Информативность прогнозов модели ARMA зависит от нескольких факторов:

        1. **Доверительные интервалы**:
            - Важно предоставлять прогнозы вместе с доверительными интервалами, которые указывают на диапазон возможных значений и степень неопределенности прогнозов. Широкие доверительные интервалы указывают на большую неопределенность.
        2. **Визуализация прогнозов**:
            - Визуальное представление прогнозов (например, графики) помогает лучше понять будущие тенденции и оценить их достоверность.
        3. **Анализ ошибок**:
            - Оценка ошибок прогнозирования, таких как среднеквадратическая ошибка (MSE) или средняя абсолютная ошибка (MAE), помогает понять точность модели и качество прогнозов.
    keywords: [88, Прогнозирование для модели ARMA, Прогнозирование, ARMA]
num89:
    text: |-
        ## 89) Оценка и тестирование модели: Предварительное тестирование на белый шум.

        Белый шум (white noise) — это чисто случайный процесс, со следующими свойствами:

        $ E(y_t) = \mu  $   

        $ Var(y_t) = const $   

        $ \gamma(\tau) = 0 $   

        По определению, данный процесс является слабо стационарным. Если же математическое ожидание равно нулю, то процесс называется белым шумом с нулевым средним.

        Если выполняется предположение о нормальном распределении значений, белый шум будет считаться строго стационарным процессом и коэффициенты автокорреляции распределены согласно нормальному закону $ N( \mu, \frac{1}{n}) $ где n — объем выборки.

        - **Предварительное тестирование на белый шум:** Перед оценкой модели ARMA необходимо проверить, является ли остаточный ряд белым шумом. Это можно сделать с помощью:
            - **Q-статистика Бокса-Пирса:** Проверяет значимость автокорреляции в остаточном ряду.
            - **Тест Бокса-Льюинга:** Обобщение Q-теста, которое учитывает корреляцию в различных лагах.
            
            Подробнее о Q-статистика Бокса-Пирса
            
            Данный тест предназначен для нахождения автокорреляции временного ряда. Он проверяет на отличие от нуля сразу несколько коэффициентов автокорреляции. Однако стоит отметить, что на практике этот тест проводится редко, так как его выборочные значения могут сильно отклоняться от распределения $ \chi^2 $.
            
            ### Выдвигаются две гипотезы:
            
            H0:данные являются случайными, т. е. представляют собой белый шум.
            
            H1:данные не являются случайными.
            
            ### Статистика выглядит следующим образом:
            
            $ Q = n \sum_{k=1}^{m} \hat{\rho}_k^2 $
            
            где n — число наблюдений, $ \hat{\rho}_k $ — автокорреляция порядка k, m — число проверяемых лагов.
            
            Если $ Q > \chi^2_{1 - \alpha, m}, где \chi^2_{1 - \alpha, m} $ — квантили распределения хи-квадрат с mmm степенями свободы, то гипотеза H_0 отвергается, а значит есть автокорреляция до mmm-го порядка во временном ряду.
            
            Данный тест предназначен для нахождения автокорреляции временного ряда. Он основан на статистике Бокса-Пирса. Стоит отметить, что этот тест не теряет своей состоятельности, даже если процесс не имеет нормального распределения (при наличии конечной дисперсии). Обычно используется при построении моделей ARIMA и применяется к остаткам полученной модели, а не к ее исходным данным. Рекомендуется применять для выборки малого объема.
            
            ### Выдвигаются две гипотезы:
            
            H0:данные являются случайными, т. е. представляют собой белый шум;H_0: \text{данные являются случайными, т. е. представляют собой белый шум;}H0:данные являются случайными, т. е. представляют собой белый шум;
            
            H1:данные не являются случайными.H_1: \text{данные не являются случайными.}H1:данные не являются случайными.
            
            ### Статистика выглядит следующим образом:
            
            Q~=n(n+2)∑k=1mρ^k2n−k,\tilde{Q} = n (n + 2) \sum_{k=1}^{m} \frac{\hat{\rho}_k^2}{n - k},Q~=n(n+2)∑k=1mn−kρ^k2,
            
            где nnn — число наблюдений, ρ^k\hat{\rho}_kρ^k — автокорреляция порядка kkk, mmm — число проверяемых лагов.
            
            Если Q~>χ1−α,m2\tilde{Q} > \chi^2_{1 - \alpha, m}Q~>χ1−α,m2, где χ1−α,m2\chi^2_{1 - \alpha, m}χ1−α,m2 — квантили распределения хи-квадрат с mmm степенями свободы, то гипотеза H0H_0H0 отвергается, а значит есть автокорреляция до mmm-го порядка во временном ряду.
    keywords: [89, тестирование на белый шум, белый шум]
num90:
    text: |-
        ## 90) Оценка модели и тестирование гипотез временного ряда.

        - После того, как вы выбрали модель для описания вашего временного ряда (например, AR, MA, ARMA, ARIMA), вам нужно оценить ее параметры и проверить, является ли модель адекватной.

        **Оценка модели:**

        1. **Метод максимального правдоподобия:** Это наиболее распространенный метод оценки параметров моделей временных рядов. Он заключается в нахождении таких значений параметров, которые максимизируют правдоподобие наблюденных данных, исходя из выбранной модели.

        $$

        $$

        1. **Другие методы:** 
            - Метод моментов.
            - Метод наименьших квадратов (OLS) для некоторых моделей.
            - Методы Байесовской оценки.

        **Тестирование гипотез:**

        1. **Тест на стационарность:** Это важно, потому что многие модели временных рядов (например, ARMA) предполагают стационарность.
            - Тест Дики-Фуллера
        2. **Тест на автокорреляцию:** Проверяет, имеет ли остаточный ряд (т.е. разность между фактическими значениями и прогнозами модели) автокорреляцию. Это свидетельствует о том, что модель не уловила все зависимости в данных, и может быть неадекватной.
            - Тест Дарбина-Уотсона
            - Тест Бреуша – Годфри
            - Метод рядов Сведа-Эйзенхарта
            - H-test
        3. **Тест на гетероскедастичность:** Проверяет, имеет ли остаточный ряд гетероскедастичность (неодинаковую дисперсию). Это может быть признаком того, что модель не учитывает изменение волатильности данных со временем.
            - **Тест Уайта:** Один из наиболее распространенных тестов на гетероскедастичность.
            - **Тест Голдфельда-Квандта:** Тест для проверки гетероскедастичности с использованием регрессии.
        4. **Тест на нелинейную зависимость (RESET):** Проверяет, есть ли в остаточном ряду нелинейные зависимости, не уловленные моделью.
    keywords: [90, Оценка модели, временного ряда, тестирование гипотез]
num81:
    text: |-
    keywords: []
