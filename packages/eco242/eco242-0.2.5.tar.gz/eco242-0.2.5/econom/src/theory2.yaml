num1:
    text: |-
        ### **1)** Линейная модель множественной регрессии. Основные предпосылки метода наименьших квадратов**.**

        Линейная модель линейной связи – это уравнение связи с несколькими переменными  $y = f(x_1, x_2, ... x_n) + ξ$ , где y – зависимая переменная, $x_n$  – независимая переменная. $y = a_0 + a_1x_1+...+a_nx_n$  – формула линейной множественной регрессии, где   – это параметры, которые вычисляются разными способами, один из них – метод наименьших квадратов.

        Также данная модель может записываться в матричном виде Y = XB+E, где Y – столбец зависимых переменных, X – матрица факторов, B – вектор коэффициентов, Е – вектор случайных ошибок. В данном случае коэффициенты матрицы B так же могут вычисляться с помощью МНК.

        МНК - математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений некоторых функций от экспериментальных входных данных.

        Предпосылки:

        1. *Математическое ожидание случайного отклонения ε равно нулю: Е(ε)=0 для всех наблюдений*
        2. *Дисперсия случайных отклонений ε постоянна:*  *для любых наблюдений i и j (гомоскедастичность)*
        3. *Случайные отклонения $\varepsilon_i$ и $\varepsilon_j$ ( $i \neq j$) не коррелируют между собой (отсутствие автокорреляции). $E(\varepsilon_i, \varepsilon_j ) = 0, i \neq j$*
        4. *Случайные отклонения должны быть статистически независимы (некоррелированы) от объясняющих переменных.*
        5. *Отсутствие мультиколлинеарности. Между объясняющий переменными отсутствует сильная линейная зависимость.*
        6. *Возмущения $\varepsilon_i$ нормально распределены $\varepsilon_i \sim N(0; \sigma^2)$.*

        Если регрессионная модель удовлетворяет условиям Гаусса-Маркова, ее параметры обладают свойствами несмещенности, эффективности и состоятельности.

        Оценки в матричном виде могут быть рассчитаны по как $B=(X^TX)^{-1}X^TY$

        Если уравнение представлено не матрицей, коэффициенты рассчитываются из системы производных по каждому коэффициенту.
    keywords:
        [
            1,
            линейная,
            множественной регрессии,
            предпосылки,
            наименьших квадратов,
        ]
num2:
    text: |-
        ### **2)** Нелинейные модели регрессии. Походы к оцениванию. Примеры

        Различают два класса нелинейной регрессии:

        1. Нелинейные по переменным, но линейные по параметрам. Примеры: полиномы > 1 степени, гиперболы. Например, полиномиальная модель: $y = \beta_0 + \beta_1x + \beta_2x^2 + … + \beta_m*x^m + \varepsilon$
        2. Нелинейные по параметрам но линейные по переменным. Примеры: Степенные, показательные функции. Например, степенная модель: $y = \beta_0*x_1^{\beta_1} * x_2^{\beta_2} * … x_m^{\beta_m}\varepsilon$
        Так как классическая модель регрессии предполагает линейный характер зависимости переменных Y и X, то перед применением МНК для оценки, нелинейные модели по параметрам линеаризовывают, это делается путём логарифмирования правой и левой части. Нелинейные по переменным же модели требуют добавления новых объясняющих переменных соответсвующим образом преобразованных. Для оценки параметров нелинейных моделей используют два подхода: Первый подход основан на линеаризации модели и заключается в том, что с помощью подходящих преобразований исходных переменных исследуемую нелинейную зависимость представляют в виде линейной между преобразованными переменными. Второй подход, основанный на применении методов нелинейной оптимизации, применяется в том случає, когда подобрать соответствующее преобразование не удается.

        Например: полиномиальная модель m-го порядка может быть приведена к линейному виду путем замены переменных $x^j = x_j^{\prime}$. После замены переменных получим линейную регрессионную модель вида $y = \beta_0 + \sigma_{j=1}^m \beta_jx_j^{\prime} + \varepsilon$
        К классу степенных функций относятся кривые спроса и предложения, производственные функции и тд. Например, производственная функция Кобба-Дугласа $Y = AK^{\alpha}L^{\beta}\varepsilon$
    keywords: [2, нелинейные, подходы к оцениванию]
num3:
    text: |-
        ### **3)** Тестирование правильности выбора спецификации: типичные ошибки спецификации модели, Тест Рамсея (тест RESET), условия применения теста.

        Типичные ошибки:

        1. Неверно выбран тип уравнения регрессии
        2. В линейное уравнение множественной регрессии включен несущественный регрессор
        3. В линейное уравнение множественной регрессии не включен существенный регрессор

        Тест Рамсея:
        RESET – тест Рамсея отвечает на вопрос, надо ли включать в регрессию степени независимых переменных (идея заключается в том, что добавление нелинейных функций $\hat Y$ не должно улучшать наших знаний относительно Y)
        Условие применения: модель - линейное уравнение множественной регрессии

        $H_0:$ спецификация модели является правильной

        $H_1:$ спецификация неправильная (не включён существенный признак)

        Для проверки сначала оценивается исходная модель, далее заново оценивается эта же модель но с добавлением вычисленных на предыдущем этапе предсказываемых значений в степени p (обычно не большой). Далее происходит оценка модели через F статистику, если она больше критического значения, значит отсутствует важный признак в модели.

        $F_{набл} = \frac{\frac{(ESS_1 - ESS_2}{p}}{\frac{ESS_2}{n-m-p}} \sim F_{крит} (p, n-m-p), 
        F_{набл} \gt F_{крит} (p, n-m-p)$
    keywords: [3, спецификации, Рамсея]
num4:
    text: |-
        ### **4)** Тестирование правильности выбора спецификации: типичные ошибки спецификации модели, Критерий Акаике, Критерий Шварца. условия применения критериев.

        Типичные ошибки:

        1. Неверно выбран тип уравнения регрессии
        2. В линейное уравнение множественной регрессии включен несущественный регрессор
        3. В линейное уравнение множественной регрессии не включен существенный регрессор

        AIC позволяет сравнивать несколько статистических моделей друг с другом для того, чтобы определить, какая из моделей лучше соответствует данным. Особенностью критерия является введение штрафа за число параметров модели.
        Статистика рассчитывается как: $AIC = ln(\frac{ESS_k}{n})+\frac{2k}{n}+1+ln(2\pi)$. При увеличении объясняющих переменных первое слагаемое в правой части уменьшается, второе – увеличивается.

        BIC - мера относительного качества моделей, учитывающая степень «подгонки» модели под данные со штрафом на используемое количество оцениваемых параметров. То есть критерий основаны на неком компромиссе между точностью и сложностью модели.

        Статистика: $BIC = ln(\frac{ESS_k}{n})+\frac{kln(n)}{n}+1+ln(2\pi)$*.* При увеличении количества объясняющих переменных первое слагаемое в правой части уменьшается, а второе – увеличивается.

        В обоих случаях предпочтение отдаётся тем моделям, где данные статистики меньше. Различаются они способом оценки баланса между переменными и ESS.

        Условия применения критериев:

        1. Сравниваемые модели построены на одном и том же наборе данных.
        2. Сравниваемые модели имеют одну и ту же объясняемую переменную.
        3. Обучающая выборка имеет бесконечный размер.
    keywords: [4, Акаике, Шварца]
num5:
    text: |-
        ### **5)** Гетероскедастичность: определение, причины, последствия. Тест Голдфеда-Квандта и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:

        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:

        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$ будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Голфеда-Квандта:

        $H_0$: Присутствует гомоскедастичность
        $H_1$: Присутствует гетероскедастичность

        1. Выборочные данные упорядочиваются по величине модуля регрессора, относительно которого есть подозрение на гетероскедастичность
        2. Выборка делится на 3 группы, где $n_1 = n_3, k <n_1<\frac{n}{2}$, k – число параметров, n – общая выборка
        3. Далее оцениваются две вспомогательные регрессии по 1 и 3 группе по которым вычисляются суммы квадратов остатков $ESS_1, ESS_3$
        4. Далее вычисляются статистики $GQ=\frac{ESS_1}{ESS_3}, GQ^{-1}=\frac{ESS_3}{ESS_1}$
        5. Определяется F распределение $F(\alpha, n_1-k, n_1-k)$
        6. Если $GQ \le F_{кр}$ и $GQ^{-1} \le F_{кр}$ то присутствует гомоскедастичность
        Данный тест применяется при маленьких выборках и нормально распределённых остатках, которые не коррелируют друг с другом, cтандартные отклонения $\sigma^2_{e_i} $пропорциональны значениям $x_i$.
    keywords: [5, Гетероскедостичность, Голдфеда-Квандта]
num6:
    text: |-
        ### **6)** Гетероскедастичность: определение, причины, последствия. Тест ранговой корреляции Спирмена и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:

        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:

        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Спирмена:

        $H_0$: Присутствует гомоскедастичность
        $H_1$: Присутствует гетероскедастичность
        1. Рассчитываются абсолютные значения остатков модели
        2. Остатки и значения фактора ранжируются (определяются их ранги)
        3. Рассчитывается ранговое значение $D^2_i = (D_x + D_e)^2$ и вычисляется сумма всех этих квадратов
        4. Далее определяется коэффициент ранговой корреляции $r = 1 - \dfrac{6\sum_{i=1}^nD^2_i}{n(n^2 -1)}$
        5. Определяется критическое значение по Стьюденту t(a, n-2)
        6. Вычисляется t статистика $t = \dfrac{r\sqrt{n-2}}{\sqrt{1-r^2}} *\sqrt{n-2}$
        7. Если t < tкр (a=0,05; n=2), то гомоскедастичность, иначе гетеро
        Данный тест рекомендуется использовать при равномерном распределении измененяемой величины.
    keywords: [6, Гетероскедостичность, Спирмена]
num7:
    text: |-
        ### **7)** Гетероскедастичность: определение, причины, последствия. Тест Тест Бреуша-Пагана и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:

        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:

        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Бреуша-Пагана:

        $H_0$: Присутствует гомоскедастичность
        $H_1$: Присутствует гетероскедастичность
        1. Рассчитываем обычную регрессию через МНК и вычисляем остатки, квадраты остатков
        2. Оценка дисперсии возмущений $\hat \sigma^2 = \dfrac{\sum_{t=1}^ne^2_t}{n}$
        3. Оценивается новая регрессия вида $\dfrac{e^2_t}{\hat \sigma_t^2} = \gamma_0 + Z_t^T \cdot \gamma +v_t$ и находится RSS
        4. Вычисляется статистика $BP=\dfrac{RSS}{2}$
        5. Если $BP < \chi^2(k)$, то гомоскедастичность, иначе гетеро
        Данный метод чаще всего использую при больших выборках
    keywords: [7, Гетероскедастичность, Бреуша-Пагана]
num8:
    text: |-
        ### **8)** Гетероскедастичность: определение, причины, последствия. Тест Глейзера и особенности его применения.

        Гетероскедастичностью называется зависимость дисперсии возмущения от периода наблюдений, т.е. $\sigma^2_{e_i} \ne \sigma^2_{e_j} \ne const$ для любых i и j.

        Причины:

        1. Эффект масштаба в пространственно-временных данных
        2. Эффект запаздывания информации
        3. Неоднородность исследуемых данных.

        Последствия:

        - Оценки параметров b не будут эффективными, особенно при малых выборках
        - Стандартные ошибки параметров $m_{b_j}$будут рассчитываться со смещением
        - Оценки t и F статистики будут ненадёжными.

        Тест Глейзера:

        $\sigma_i = a_0 + a_1*x_i^{\gamma} +w_i$

        $H_0$: Присутствует гомоскедастичность (a_1=0)

        $H_1$: Присутствует гетероскедастичность $(a1 \neq 0)$

        1. Вычисляем абсолютные значения остатков для обычной модели при МНК
        2. Определяем $t_{кр} = t(\alpha, n-2)$
        3. Строим МНК оценки для моделей $|\varepsilon_i|=a_0 +a_1 \cdot x_i^{\gamma}$, для $\gamma$ от -1 до 1 с шагом 0.5, если хотя бы для одной модели t статистика a1 оказалась значимость, значит есть гетероскедастичность, иначе нет

        Данный тест способен не только выявить гетероскедастичность, но и конкретный вид зависимости этих остатков от каждой независимой переменной
    keywords: [8, Гетероскедастичность, Глейзера]
num9:
    text: |-
        ### **9)** Способы корректировки гетероскедастичности: взвешенный метод наименьших квадратов (ВМНК) и особенности его применения.

        Способы:

        1. Взвешенный метод наименьших квадратов (ВМНК)
        2. Доступный взвешенный метод наименьших квадратов (ДВМНК)
        3. Обобщённый метод наименьших квадратов (ОМНК)

        Перечисленные методы нацелены на преобразование переменных таким образом, чтобы в спецификации преобразованной модели случайное возмущение удовлетворяло предпосылкам Гаусса-Маркова

        ВМНК:

        Применяется, если бы нам были известны дисперсии всех ошибок.

        Правая и левая часть уравнения регрессии $y_i = \beta_0 + \beta_1*x_i + \varepsilon_i$ делятся на известное $\sigma_i$. 

        Модель примет вид:

        $$
        \dfrac{Y_i}{\sigma_i}=\beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{X_{2i}}{\sigma_i}+...+\beta_k \dfrac{X_{ki}}{\sigma_i}+\dfrac{u_i}{\sigma_i}
        $$

        Её можно привести к обычному линейному виду заменив новые коэффициенты:
        $$
        Y^*_i=\beta_0 z_i + \beta_1 X^*_{2i}+...+\beta_kX^*_{ki}+u^*_{i}  
        $$

        Данный вектор случайных отклонений $u_i^*$ удовлетворяет условию гомоскедастичности.
        В случае неизвестных дисперсий положим $\sigma_i = \sqrt{X_i}$ и проделаем всё то же самое
    keywords: [9, корректировки гетероскедастичности, ВМНК]
num10:
    text: |-
        ### **10) Автокорреляция: определение, причины, последствия. Тест Дарбина-Уотсона и особенности его применения.**

        Автокорреляция – это наличие сильной корреляционной зависимости между последовательными уровнями ряда динамики.

        Автокорреляция — это понятие математической статистики, которое характеризует степень статистической взаимосвязи между элементами данных одного временного ряда.

        **Причинами автокорреляции являются:**

        - ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);
        - ошибки измерений*;*
        - характер наблюдений (например, данные временных рядов).
        - Инертность экономических показателей
        - The Cobweb effect (паутинообразный эффект) (во многих отраслях производства экономические показатели реагируют на изменение экономических условий с запаздыванием);
        - «Манипулирование данными». Сглаживание данных.

        **Последствия автокорреляции:**

        - Хотя оценки МНК коэффициентов регрессии останутся несмещенными, они уже не будут эффективными
        - Оценки МНК для стандартных отклонений коэффициентов регрессии будут смещенными, чаще всего вниз, т.е будут заниженными.
        - Статистики t и F будут неадекватными. Следствием заниженности оценок стандартных отклонений коэффициентов является завышенность t – статистик.

        **Тест Дарбина-Уотсона:**

        $H_0$: между остатками нет корреляции.

        $H_1$: остатки автокоррелированы.

        Формула для вычисления статистики Дарбина-Уотсона **DW** по остаткам регрессии:
        $$
        DW = \frac {\sum_{t=2}^{n}(e_t - e_{t-1})^2}{\sum_{t=1}^{n}e_t^2} 
        $$

        **Предпосылки:**

        - случайное возмущение $\varepsilon$  распределено нормально;
        - не подвержено гетероскедастичности;
        - модель не включает лаговые значения эндогенных переменных.
        При $DW \rightarrow 2$, автокорреляции нет.

        При $DW \rightarrow 0$, положительная автокорреляции.

        При $DW \rightarrow 4$, отрицательная автокорреляции.

        На практике проверка гипотезы **$H_0$**  об отсутствии автокорреляции остатков осуществляется с помощью сравнения статистики **DW** с теоретическими значениями **$d_l$** и **$d_u$** для заданного числа наблюдений n, числа независимых переменных модели k и уровня значимости α:

        - 0 < *DW* < *$d_l$* - гипотеза $H_0$  отвергается, есть положительная автокорреляция;
        - *dl* < *DW* < *$d_u$* - зона неопределённости;
        - $*d_u*$ < *DW* < **4 - $*d_u*$ - гипотеза $H_0$  не отвергается, автокорреляции нет;
        - 4 - $*d_u*$ < *DW* < **4 - $*d_l*$ - зона неопределённости;
        - 4 - $*d_l*$ < *DW* < **4 - гипотеза $H_0$ отвергается, есть отрицательная автокорреляция.

        **При использовании теста *DW* следует учитывать следующие ограничения:**

        - он применим лишь для модели с ненулевым свободным членом;
        - остатки должны описываться авторегрессионной моделью первого порядка;
        - регрессоры являются нестохастическими;
        - модель не подвержена гетероскедастичности;
        - применяется для выявления автокорреляции только между регрессионными остатками в последовательных наблюдениях.
    keywords: [10, Дарбина-Уотсона, Автокорреляция]
num11:
    text: |-
        ### 11) Автокорреляция: определение, причины, последствия. Тест Бреуша – Годфри и особенности его применения.

        Зависимость случайных возмущений от времени называется автокорреляцией*.*

        Автокорреляция – это наличие сильной корреляционной зависимости между последовательными уровнями ряда динамики.

        Автокорреляция — это понятие математической статистики, которое характеризует степень статистической взаимосвязи между элементами данных одного временного ряда.

        **Причинами автокорреляции являются:**

        - ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);
        - ошибки измерений*;*
        - характер наблюдений (например, данные временных рядов).
        - Инертность экономических показателей
        - The Cobweb effect (паутинообразный эффект) (во многих отраслях производства экономические показатели реагируют на изменение экономических условий с запаздыванием);
        - «Манипулирование данными». Сглаживание данных.

        **Последствия автокорреляции:**

        - Хотя оценки МНК коэффициентов регрессии останутся несмещенными, они уж не будут эффективными
        - Оценки МНК для стандартных отклонений коэффициентов регрессии будут смещенными, чаще всего вниз, т.е будут заниженными.
        - Статистики t и F будут неадекватными. Следствием заниженности оценок стандартных отклонений коэффициентов является завышенность t – статистик.

        **Тест Бреуша – Годфри:**

        $H_0$: нет автокррреляции в любом порядке, меньшем или равном $\rho$.

        $H_1$: сущесьвует автокорреляция в некотором порядке меньщем или равном $\rho$.

        Особенность данного теста заключается в том, что его можно использовать практически всегда, в отличие от, например, критерия Дарбина — Уотсона или h-теста Дарбина. 
        Кроме того, указанные тесты проверяют только автокорреляцию первого порядка, 
        тогда как тест Бреуша — Годфри позволяет проверить автокорреляцию любого порядка.

        **Формула**
        $$
        e_t = x^T\cdot \beta + \sum_{i=1}^{p}a_i \cdot e_{t-i} + u_t
        $$

        Для проверки нулевой гипотезы используем значение статистики $(n - k) \cdot R^2$,
        где n - число наблюдений для исходной модели. Фактически из объема исходной выборки
        вычитается наблюдения, на которые сократилась наша выборка вследствие построения вспомогательной регрессии теста Бреуша-Годфри с лаговыми переменными, 
        $R^2$ - коэффициент детерминации вспомогательной модели. 
        С помощью таблицы критических значений  $\chi^2$-распределения находим критическую точку $\chi^2_{0,05}(k)$. Если значение наблюдаемой статистики будет превышать значение критической точки – $H_0$ отклоняется при этом уровне значимости.

        Данный тест основан на следующей идее: если имеется корреляция между соседними наблюдениями, то естественно ожидать, что в уравнении $e_t=γ+ρe_{t-1}+v$, 
        (где $e_t$ – остатки регрессии, полученные обычным методом наименьших квадратов) коэффициент ρ окажется статистически значимым.
        Если t статистика этого коэффициента окажется значимой, значит автокорреляция есть.
    keywords: [11, Бреуша – Годфри, Автокорреляция]
num12:
    text: |-
        ### 12) Автокорреляция: определение, причины, последствия. H – тест и особенности его применения.
        Зависимость случайных возмущений от времени называется автокорреляцией*.*

        Автокорреляция – это наличие сильной корреляционной зависимости между последовательными уровнями ряда динамики.

        Автокорреляция — это понятие математической статистики, которое характеризует степень статистической взаимосвязи между элементами данных одного временного ряда.

        **Причинами автокорреляции являются:**

        - ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);
        - ошибки измерений*;*
        - характер наблюдений (например, данные временных рядов).
        - Инертность экономических показателей
        - The Cobweb effect (паутинообразный эффект) (во многих отраслях производства экономические показатели реагируют на изменение экономических условий с запаздыванием);
        - «Манипулирование данными». Сглаживание данных.

        **Последствия автокорреляции:**

        - Хотя оценки МНК коэффициентов регрессии останутся несмещенными, они уж не будут эффективными
        - Оценки МНК для стандартных отклонений коэффициентов регрессии будут смещенными, чаще всего вниз, т.е будут заниженными.
        - Статистики t и F будут неадекватными. Следствием заниженности оценок стандартных отклонений коэффициентов является завышенность t – статистик.
        **H-тест:**

        $H_0$: между остатками нет корреляции.

        $H_1$: остатки автокоррелированы.

        H-тест используется для проверки наличия автокорреляции в остатках регрессионной модели. Основным принципом теста является сравнение остатков регрессии с их лагами. Если значения тестовой статистики находятся в предполагаемом диапазоне, то автокорреляция отсутствует.

        H-статистика Дарбина применима тогда, когда среди объясняющих регрессоров есть $Y_{t-1}$. На первом шаге методом МНК строится регрессия. Затем критерий h Дарбина применяется для выявления автокорреляции остатков в модели с распределёнными лагами:1

        $$
        h = (1 - \frac {1}{2}\cdot DW) \cdot \sqrt{\frac {n}{1 - n \cdot Var(\hat \gamma)}}
        $$
        где
        - n - число наблюдений
        - $Var(\hat \gamma)$ - оценка дисперсии коэффициента при лаговой результативной переменной
    keywords: [12, Автокорреляция, H-тест]
num13:
    text: |-
        ### 13) Автокорреляция: определение, причины, последствия. Метод рядов Сведа-Эйзенхарта и особенности его применения.
        Зависимость случайных возмущений от времени называется автокорреляцией*.*

        Автокорреляция – это наличие сильной корреляционной зависимости между последовательными уровнями ряда динамики.

        Автокорреляция — это понятие математической статистики, которое характеризует степень статистической взаимосвязи между элементами данных одного временного ряда.

        **Причинами автокорреляции являются:**

        - ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);
        - ошибки измерений*;*
        - характер наблюдений (например, данные временных рядов).
        - Инертность экономических показателей
        - The Cobweb effect (паутинообразный эффект) (во многих отраслях производства экономические показатели реагируют на изменение экономических условий с запаздыванием);
        - «Манипулирование данными». Сглаживание данных.

        **Последствия автокорреляции:**

        - Хотя оценки МНК коэффициентов регрессии останутся несмещенными, они уж не будут эффективными
        - Оценки МНК для стандартных отклонений коэффициентов регрессии будут смещенными, чаще всего вниз, т.е будут заниженными.
        - Статистики t и F будут неадекватными. Следствием заниженности оценок стандартных отклонений коэффициентов является завышенность t – статистик.

        **Метод рядов:**

        $H_0$: отсутствие автокорреляции.

        $H_1$: наличие автокорреляции.

        При использовании метода рядов последовательно определяются знаки отклонений остатков.

        Ряд определяется как непрерывная последовательность одинаковых знаков. Количество знаков в ряду называется длиной ряда.

        Визуальное распределение знаков свидетельствует о неслучайном характере связей между отклонениями. Если рядов слишком мало по сравнению с количеством наблюдений n, то вполне вероятна положительная автокорреляция. Если же рядов слишком много, то вероятна отрицательная автокорреляция.

        Для более детального анализа предлагается следующая процедура. 

        Пусть:
        - n - объем выборки;
        - $n_1$ - общее количество знаков "+" при n наблюдениях (количество положительных отклонений е);
        - $n_2$ - общее количество знаков " - " при n наблюдениях (количество отрицательных отклонений е);
        - k - количество рядов.

        При достаточно большом количестве наблюдений ($n_1$ > 10, $n_2$ > 10) и отсутствии автокорреляции мы имеем асимптотически нормальное распределение с:

        $M(k) = \frac {2\cdot n_1 \cdot n_2}{n_1 + n_2} +1$; $D(k) = \frac {2 \cdot n_1 \cdot n_2 \cdot (2\cdot n_1 \cdot n_2 - n_1 - n_2)}{(n_1 + n_2)^2 \cdot (n_1 + n_2 -1)}$

        Тогда, если:

        $M(k) - u_{\gamma / 2}\cdot \sqrt{D(k)} < k < M(k) + u_{\gamma/2} \cdot \sqrt{D(k)}$, то гипотеза $H_0$ об отсутствии автокорреляции не отклоняется, если k лежит левее данного интервала, то есть положительная автокорреляция, а если правее - то отрицательная автокорреляция.

        Здесь $\gamma$ - уровень значимости гипотезы $H_0$ об отсутствии автокорреляции. Значение $u_{\gamma/2}$ определяются по таблице значений фунеции Лапласса $Ф(u) = \frac {1}{\sqrt{2 \pi}} \cdot \int_0^u e^{\frac{-t^2}{2}}dt$ и состовляют: для $\gamma = 0.01$ $u_{\gamma/2} = 2.575$, для $\gamma = 0.05$ $u_{\gamma/2} = 1.645$.
    keywords: [13, Автокорреляция, Сведа-Эйзенхарта]
num14:
    text: |-
        ### 14) Модель с автокорреляцией случайного возмущения. Оценка моделей с авторегрессией.

        Модель с автокорреляцией случайного возмущения — это модель, в которой возмущение (случайная составляющая) модели в момент времени t зависит от возмущений в предыдущие моменты наблюдений. Для учёта автокорреляции возмущений используются модели авторегрессии. Простейшей из таких моделей является модель авторегрессионного процесса первого порядка. 

        $Y_t = \beta_1 + \beta_2 \cdot X_t + e_t$, $e_t = \rho \cdot e_{t-1} + \varepsilon_t$

        Для того, чтобы оценить модели с авторегрессией, необходимо знать $\rho$, но для начала сделаем преобразования, чтобы соблюсти условия Гаусса-Маркова.
        Получим уравнение: $Y_t - \rho \cdot Y_{t-1} = \beta_1 \cdot (1 - \rho) + \beta_ 2 \cdot (X_t - X_{t-1}) + (e_t - e_{t-1})$

        Тут остатки уже свободны от автокорреляции. Далее при замене зависимой и независимых переменных мы перейдём к знакомой линейной модели, но потеряем первое значение.
    keywords: [14, авторегрессией, автокорреляцией случайного возмущения]
num15:
    text: |-
        ### 15) Процедура Кохрейна-Оркатта.

        Процедура Кохрейна-Оркатта — это итерационная процедура оценки параметра авторегрессии.

        **Применение:**

        - По выборочным данным выполняется настройка модели, и вычисляется вектор остатков регрессии. $\varepsilon = (\varepsilon_1, \varepsilon_2, ..., \varepsilon_n) ^T$
        - По остаткам регрессии оценивается модель авторегрессии первого порядка.

        $$
        \varepsilon_t = \rho \cdot \varepsilon_{t-1} + v_t
        $$

        - С оценкой $\hat \rho$ выполняются преобразования переменных:

        $Y_t^* = Y_t - \hat \rho \cdot Y_{t-1}$, $X_t^* = X_t - \hat \rho \cdot X_{t-1}$, $t = 1,2,...,n$

        - По скорректированным выборочным данным определяются МНК - оценки параметров $\hat \alpha^*$и $\hat \beta^*$. Оценка параметра $\hat \beta^*$ непосредственно используется в исходной модели, 
        т.к. $\hat \beta = \hat \beta^*$, оценка параметра  вычисляется по формуле $\hat \alpha = \frac{\hat \alpha^*}{1 - \hat \rho}$.
        - По оценкам параметров $\hat \alpha$ и $\hat \beta$ вычисляется оценка эндогенной переменной 
        $\tilde{Y}_t = \hat \alpha + \hat \beta \cdot X_t$
        - Строится новый вектор остатков, и процедура повторяется с п.2.

        Итерационный процесс заканчивается при условии совпадения оценок на последней и предпоследней итерациях с заданной степенью точности ($\delta)$.

        Оценка (прогноз) эндогенной переменной *Y*, в рамках метода Кохрейна-Оркатта, выполняется по формуле:

        $\hat Y_t = \hat \alpha + \hat \beta \cdot X_t + \hat \rho \cdot e_{t-1} = \tilde Y + \hat \rho \cdot e+{t-1}$, $t = 1,2,...,n$
    keywords: [15, Кохрейна-Оркатта]
num16:
    text: |-
        ### 16) Процедура Хилдрета – Лу.

        Процедура Хилдрета – Лу - это последовательная процедура оценки параметра авторегрессии путём перебора значений ρ в пределах от -1 до 1, и подстановкой в оцениваемое уравнение $Y_t-\rho Y_{t-1} = \beta_1 \cdot (1-\rho)+\beta_2 \cdot (X_t-X_{t-1})+\varepsilon_t$. Нужная оценка выбирается при наименьшем значении RSS.
    keywords: [16, Хилдрета – Лу]
num17:
    text: |-
        ### 17) Оценка влияния факторов, включенных в модель. Коэффициент эластичности, Бета-коэффициент, Дельта – коэффициент.

        Учитывая, что коэффициент регрессии невозможно использовать для непосредственной оценки влияния факторов на зависимую переменную из-за различия единиц измерения и разной колеблемости факторов, используем коэффициенты эластичности, бета-коэффициенты и дельта коэффициенты.

        $E_i = b_i \cdot \frac {\bar x_i}{\bar y_i}$ — коэффициент эластичности показывает на сколько процентов изменяется в среднем Y при увеличении только фактора X_i на один процент.

        $B_i = b_i \cdot \frac {S_{x_i}}{S_y}$ — бета-коэффициент (можно упорядочить факторы по степени их влияния на Y: больший модуль бета-коэффициента соответствует более сильному влиянию)

        $\Delta_i = \frac {B_i \cdot r_{y, x_j}}{R^2}$ — дельта коэффициент показывает долю влияния фактора $X_i$ на результат Y в суммарном влиянии всех факторов, включенных в модель.
    keywords:
        [
            17,
            влияния факторов,
            эластичности,
            Бета-коэффициент,
            Дельта – коэффициент,
        ]
num18:
    text: |-
        ### 18) Мультиколлинеарность: понятие, причины и последствия.

        **Мультиколлинеарность** — наличие  сильной линейной взаимосвязи между двумя и более объясняющими переменными.

        **Совершенная мультиколлинеарность** — наличие функциональной зависимости между двумя  объясняющими переменными.

        **Виды мультиколлинеарности:**

        - строгая мультиколлинеарность – наличие линейной функциональной связи между независимыми переменными
        - нестрогая мультиколлинеарность – наличие сильной линейной корреляционной связи между независимыми переменными.
        **Причины мультиколлинеарности:**

        - Ошибочное включение в уравнение двух или более линейно зависимых переменных;
        - В модели использованы факторные признаки, являющиеся составными элементами друг друга;
        - Исходные данные представлены временными рядами, имеющими одинаковые тенденции.

        **Последствия мультиколлинеарности:**

        - Большие значения дисперсии оценок, что затрудняет нахождение всех интервальных оценок;
        - Уменьшается t-статистика Стъюдента;
        - Оценки, полученные по методу наименьших квадратов станут неустойчивыми;
        - Знак у коэффициентов регрессии может получиться неверным.
    keywords: [18, Мультиколлинеарность]
num19:
    text: |-
        ### 19) Выявление мультиколлинеарности: коэффициент увеличения дисперсии (VIF –тест)

        1. Строятся регрессии для каждого j-ого регрессора $X_j$ по всем остальным регрессорам;
        2. Для изменения эффекта мильтиколлинеарности используется показатель VIF-факторин инфляции вариации: $VIF_j = \frac {1}{1-R^2_j}$, $j = 1, 2, …, k$;
        3. Если $VIF_j > 10$ - данный регрессор приводит к мультиколлинеарности.
    keywords: [19, VIF – тест, Выявление мультиколлинеарности]
num20:
    text: |-
        ### 20) Выявление мультиколлинеарности: Алгоритм Фаррара-Глобера

        Содержит 3 вида статистических критериев

        - всего массива $(\chi^2)$ (п. а)
        - каждой переменной с другими переменными (F-критерий) (п. б)
        - каждой пары переменных (п. в)

        a:

        1. построить корреляционную матрицу, найти её определитель;
        2. $FG_{набл} = -[n - 1 - \frac {1}{6} (2k + 5)] ln(det[R])$;
        3. $N = C^2_{k-1} = \frac {(k-1)(k-2)}{2}$
        4. фактическое значение сравнивается с табличным $\chi^2_{\alpha; 0,5k(k-1)}$;
        5. если $FG_{набл} ≥ \chi^2$ - мультиколлинеарность.
        б:

        1. вычисляется обратная матрица $C = R^{-1}$;
        2. $F_j = (c_{jj} - 1) \frac {n-k-1}{k}$, где $c_{jj}$ - диагональные элементы С;
        3. фактические значения сравниваются с табличными: $\nu_1 = k, \nu_2 = (n - k -1)$;
        4. если $F_j > F_{табл}$, то $X_j$ имеет тесную связь с другими регрессорами.

        в:

        1. найти частный коэффициент корреляции: $r_{ij} = \frac {-c_{ij}}{\sqrt{c_{ii} * c{jj}}}$;
        2. $t_{ij} = \frac {r_{ij}\sqrt{n - k - 1}}{\sqrt{1-r^2_{ij}}}$;
        3. если $|t_{ij} > t_{табл}(\alpha; (n - k - 1))$ - мультиколлинеарность.
    keywords: [Фаррара-Глобера, Выявление мультиколлинеарности]
num21:
    text: |-
        ### 21) Построение гребневой регрессии. Суть регуляризации.

        Применение ридж-регрессии предполагает корректировку элементов главной диагонали матрицы $X^TX$ на некоторую произвольно задаваемую положительную величину: $\tau = \frac {mSS_e}{n - m - 1} \frac {1}{a^{*T} a^*}$, где $\tau$ - количество параметров (без учета свободного члена) в исходной модели регрессии, $SS_e$ - остаточная сумма квадратов, полученная по исходной модели регрессии без корректировки на мультиколлинеарность, $a^*$ - вектор-столбец коэффициентов регрессии, преобразованных по формуле:  $a^*_j = a_j \sqrt {\sum (x_j - \overline x_j) ^ 2}$, $a_j$ - параметр при переменной $x_j$ в исходной модели регрессии. 

        После выбора величины $\tau$ формула для оценки параметров будет иметь вид: $a_{\tau} = (X^T_{\tau}X_{\tau} + \tau I) ^ {-1}X^T_{\tau}Y_{\tau}$ (1), где $I$ - единичная матрица, $X_{\tau}$ - матрица значений независимых переменных: исходных или преобразованных по формуле $x_{\tau j} = \frac {x_j - \overline x_j}{\sqrt {\sum {(x_j - \overline x_j) ^ 2}}}$, $Y_{\tau}$ - вектор значений зависимой переменной: исходных или преобразованных по формуле $y_{\tau} = y - \overline {y}$.
        В этом случае после оценки параметров необходимо перейти к регрессии по исходным переменным, используя соотношения:

        $a_j = \frac {a_{\tau j}}{\sqrt {\sum {(x_j - \overline x_j) ^ 2}}}, j = 1, 2, …, m; a_0 = \overline y - \sum {a_j \overline x_j}$

        Оценки параметров регрессии, полученные с помощью формулы (1), будут смещенными. Однако, так как определитель матрицы $(X^TX + \tau I)$ больше определителя матрицы $X^TX$, дисперсия оценок параметров регрессии уменьшится, что положительно повлияет на прогнозные свойства модели.
    keywords: [гребневой регрессии, регуляризации]
num22:
    text: |-
        ### 22) Алгоритм пошаговой регрессии.

        1. Рассматриваются все возможные простые регрессии; независимая переменная, объясняющая наибольшую значащую долю вариации $\gamma$ - первая включается в уравнение регрессии
        2. Следующая вводимая переменная - та, $\gamma$ которой привносит наибольший значащий вклад в регрессионную сумму квадратов, опред. с помощью F-теста
        3. После включения доп. переменных в уравнение, отдельный вклад в регрессионную сумму квадратов каждой из переменных, включенных в уравнение, проверяется на значимость с помощью F-теста; если полученное значение < чем F для исключения ⇒ переменная исключается из уравнения
        4. Этапы 2 и 3 повторяются пока все возможные добавления не окажутся незначимыми, а все возможные исключения - значимыми; в этот момент процедура выбора заканчивается
    keywords: [22, пошаговой регрессии]
num23:
    text: |-
        ### 23) Метод главных компонент (PCA) как радикальный метод борьбы с мультиколлинеарностью

        Модель: $y_j = \sum_{j=1}^n a_{jr} f_r$, где $f_r$ - r-ая главная компонента, $a_{jr}$ - весовой коэффициент r-ой компоненты на j-ой переменной, $y_j$ - центрированное значение j-ого признака. 

        Алгоритм: 

        1. Представление исходных данных в виде матрицы Х, размерностью n x m, где n - число объектов наблюдения, m- число признаков наблюдения. 
        2. Стандартизация матрицы исходных данных и получение матрицы стандартизированных значений признаков (Z): $z_{ij} = \frac {x_{ij} - \overline {x_j}}{\sigma_j}$
        3. Расчет матрицы парных коэффициентов корреляции: $R = \frac {1}{n}Z^’Z$
        4. Определение матрицы собственных чисел $\Lambda$ путем решения характеристического уравнения $|R - \Lambda E| = 0$
        5. Определение матрицы собственных векторов U путем решения матричного уравнения $(R - \Lambda EU) = 0$
        6. Определение матрицы нормированных собственных векторов V путем преобразования ненормированных собственных векторов: $V = \frac {U_j}{|Uj|}$
        7. Определение матрицы факторного отображения A по формуле $A = V * \Lambda ^ \frac {1}{2}$, элементы которой представляют собой частные коэффициенты корреляции между исходными переменными и главными компонентами
        8. Определение матрицы значений главных компонент по формуле $F = A^{-1}Z^’$
        $$ U^TRU = \Lambda =\begin{pmatrix}
        \lambda_1 & 0 & ... & 0\\
        0 & \lambda_2 & ... & 0 \\ ...& ... & ... & ... \\ 0 & 0 & 0 & \lambda_n
        \end{pmatrix}$$
    keywords: [23, Метод главных компонент, PCA]
num24:
    text: |-
        ### 24) Фиктивная переменная и правило её использования

        Фиктивная переменная - переменная бинарного типа, принимающая значения 0 или 1, и позволяющая отражать влияние качественных переменных в анализе

        $D=\begin{cases} 1&\text{- если фактор действует} \\ 0 &\text{- не действует} \end{cases}$

        Правило использования: если качественная переменная имеет k альтернативных значений, то при моделировании используется только k-1 фиктивная переменная
    keywords: [24, Фиктивная переменная]
num25:
    text: |-
        ### 25) Модель дисперсионного анализа.

        ANOVA-модель или модель дисперсионного анализа - модель, в которой в качестве объясняющей переменной находится фиктивная переменная. 

        Вид: $y = \beta + \gamma * D + \varepsilon$, $\gamma$ - коэффициент регрессии при фиктивной переменной. 

        Значимость $\gamma$ проверяется с помощью t-критерия Стьюдента. Если $\gamma$ значим, то фиктивная переменная D влияет на $y$.

        Пример: 

        y - заработная плата, D - наличие высшего образования (1 - есть, 0 - нет).

        Получаем уравнения вида:

        1. если у сотрудника есть высшее образование: $M(y|D=1) = \beta + \gamma$
        2. если у сотрудника нет его: $M(y|D=0) = \beta$
    keywords: [25, дисперсионного анализа]
num26:
    text: |-
        ### 26) Модель ковариационного анализа.

        ANCOVA-модель или модель ковариационного анализа - модель, в которой в качестве объясняющих переменных используют количественные и качественные переменные.

        Вид: $y = \beta_1 + \beta_2 * x + \gamma * D + \varepsilon$, $\gamma$ - коэффициент регрессии при фиктивной переменной. 

        Пример: 

        y - заработная плата, x - стаж работы, D - пол сотрудника(1 - мужчина, 0 - женщина).

        Получаем уравнения вида:

        1. если сотрудник женщина: $M(y|D=0) = \beta_0 + \beta_1 * x$
        2. если сотрудник мужчина: $M(y|D=1) = \beta_0 + \beta_1 * x + \gamma$ 

        Значимость $\gamma$ проверяется с помощью t-критерия Стьюдента. Если $\gamma$ значим, то присутствует половая дифференциация  по оплате труда.
    keywords: [26, ковариационного анализа]
num27:
    text: |-
        ### 27) Фиктивные переменные в сезонном анализе

        Предположим, что задача состоит в исследовании временного ряда $x_{ij}$, где i - номер сезона, L - число сезонов в году, j - номер года, m - кол-во лет. Кол-во уровней исходного временного ряда равно $n = L*m$ 

        При построении модели регрессии с переменной структурой необходимо учитывать, что число сезонных фиктивных переменных должно быть на единицу меньше сезонов внутр. года, т.е. $L-1$

        Пусть модель поквартальная, тогда значения ******фиктивных переменных D2 D3 D4 прим. значения:

        $\begin{matrix} Кварт & D2 & D3 & D4\\ 1 & 0 & 0 & 0\\2 & 1 & 0 & 0\\3 & 0 & 1 & 0\\4 & 0 & 0 & 1\end{matrix}$ 

        Общий вид: $y_t =  \beta_0 + \beta_1*t + \sigma_2 * Д2 + \sigma_3* Д3 + \sigma_4 * Д4 + \Sigma t$ 

        Базисная модель: $y_t = \beta_0 + \beta_1*t + \Sigma t$
        Коэффициент  $\beta_1$ характеризует среднее абсолютное изменение уровней временного ряда под влиянием свободной тенденции. Сезонная компонента рассчитывается как разность между средним значением свободных членов всех частных моделей регрессии и значением постоянного члена одной из моделей

        $\beta_0 = \dfrac{\beta_0 + (\beta_0 + \sigma_2) + (\beta_0 + \sigma_3) + (\beta_0 + \sigma_4)}{4}$

        Оценки сезонного отклонения для 1 квартала: $(\beta_0 - \bar{\beta_0})$

        Для 2-4 кварталов: Для 2-4 кварталов: $(\beta_0 + \sigma_i - \bar{\beta_0}), \text{где i} = \bar{2},\bar{4}$
    keywords: [27, Фиктивные переменные в сезонном анализе]
num28:
    text: |-
        ### 28) Фиктивная переменная сдвига: спецификация регрессионной модели с фиктивной переменной сдвига; экономический смысл параметра при фиктивной переменной; смысл названия.

        Рассмотрим в качестве формы уравнения регрессии линейную функцию. Для простоты возьмем в качестве факторов одну количественную переменную $*х_1*$ и одну фиктивную переменную $*z_{11}*$:

        $$
        y = a + b_1x_1 + c_{11}z_{11} + \varepsilon
        $$

        Из этого уравнения следует, что при $*z_{11} = 1*$ результат $(у)$ раввен:

        $$
        \begin{equation}
        y = (a + c_{11})+ b_1x_1 + \varepsilon
        \end{equation}
        $$

        А при $*z_{11} = 0*$ результат $(у)$ равен:

        $$
        \begin{equation}
        y = a + b_1x_1 + \varepsilon
        \end{equation}
        $$

        Сравнивая два полученных уравнения $(1)$ и $(2)$, видим, что они различаются величиной свободного члена. То есть для одного уравнения результат всегда в среднем будет на $*с_{11}*$ выше или ниже, чем для другого, при одном значении регрессора.

        Графически эта ситуация соответствует двум параллельным прямым. Отметим, что коэффициент $*b_1*$ при регрессоре остается неизменным. То есть изменение фактора $*x_1*$оказывает одинаковое влияние на результат при разных значениях нулевого члена.

        Так как изменение значения фиктивной переменной в модели приводит к изменению значения результата на некую среднюю величину, не зависящую от значений признаков модели, такую переменную называют ***фиктивной переменной сдвига.*** Изменение ее значения приводит к переходу от одной параллельной прямой к другой.

        Экономический смысл: фиктивные переменные сдвига активно применяются при исследовании сезонных колебаний.
    keywords: [28, сдвига, Фиктивная переменная сдвига]
num29:
    text: |-
        ### 29) Фиктивная переменная наклона: спецификация регрессионной модели с фиктивной переменной наклона; экономический смысл параметра при фиктивной переменной; смысл названия.

        Рассмотрим другую ситуацию: коэффициент регрессии при $x_i$ зависит от значения фиктивной переменной. То есть можно записать:

        Если $z=0:$

        $$
        \hat y = a + b_{11}x_1
        $$

        Если $z = 1:$

        $$
        \hat y = a + b_{12}x_1
        $$

        В таком случае говорят, что имеют место структурные изменения в исследуемой зависимости. Для их учета в уравнении регрессии фиктивную переменную вводят как сомножитель при $x_i$:

        $$
        \hat y = a + b_1x_1 + d_{111}x_1z_{11}
        $$

        Так как параметр $*d*$ объединяет две переменные – $*х_1*$ и $*z_{11}*$, он имеет тройной индекс – $*d_{111}$.*

        Если рассмотреть уравнение для $*z_{11} = 1*$ и для $*z_{11} = 0*$, получим соответственно:

        $$
        \hat y = a + (b_{1} + d_{111})x_1
        $$

        $$
        \hat y = a + b_{1}x_1
        $$

        Графически модель можно представить в виде двух прямых с разным углом наклона, отражающих зависимость результата от фактора при разных значениях фиктивной переменной. Так как речь идет о фиктивной переменной, включение которой позволяет изменить угол наклона прямой, такую переменную называют ***фиктивной переменной наклона.***
        Экономический смысл: Фиктивные переменные наклона могут помочь выявить структурные различия в моделях для разных подвыборок.
    keywords: [29, наклона, Фиктивная переменная наклона]
num30:
    text: |-
        ### 30) Определение структурных изменений в экономике: использование фиктивных переменных, тест Чоу.

        В экономике **структурные изменения** — это **сдвиг или изменение основных способов функционирования рынка или экономики**.

        Такие изменения могут быть вызваны такими факторами, как экономическое развитие, глобальные сдвиги в капитале и рабочей силе, изменения в доступности ресурсов вследствие войны или стихийных бедствий, открытия или истощения природных ресурсов или изменения в политической системе. 
        При изучении связи между показателями результативный признак, являющийся **количественной переменной**, может зависеть не только от количественных факторов, но и от **неколичественных факторных признаков.** В таких случаях применяют **фиктивные переменные.**

        **Тест Чоу:**
        Применяем, когда есть подозрения, что исходная выборка состоит из 2 или более разных подвыборок, которые отличаются, например, значением качественной переменной.

        1) Мы разделили наши данные на две группы на основе некоторой структурной точки разрыва и подобрали к каждой группе следующие модели регрессии:

        $$
        y = a_0 + a_1x_1 + a_2x_2 + \varepsilon_1 
        $$

        $$
        y = b_0 + b_1x_1 + b_2x_2 + \varepsilon_2 
        $$

        **Формулируем гипотезы:**

        $H_0:$   $a_0 = b_0; a_1 = b_1; a_2 = b_2$

        $H_1: \exists i : a_i \neq b_i$

        **Рассчитываем F-статистику:**

        $$
        F_{расч} = \frac{\frac{RSS_T - RSS_1 - RSS_2}{m+1}}{\frac{RSS_1 + RSS_2}{n - 2m-2}}
        $$

        $RSS_T$ - сумма квадратов остатков регрессии для всей выборки;

        $RSS_1$ - сумма квадратов остатков регрессии по первой части;

        $RSS_2$ - сумма квадратов остатков регрессии по второй части;

        $n$ - число наблюдений;

        $m$ - число параметров в модели.

        $F_{крит}$ соответствует F-распределению с  $(m+1)$ и $(n - 2m - 2)$ степенями свободы, и уровнем значимости $\alpha$.

        **Вывод:** Если $F_{расч}$ > $F_{крит}$, мы можем отклонить $H_0$ на уровне значимости $\alpha$ и сделать вывод, что в данных есть точка структурного разрыва.
    keywords: [30, изменений в экономике, тест Чоу]
