N_SAMPLES = 10
LIBS_PER_SAMP = 2
RUNS_PER_LIB = 1
TOTAL_COV = 20
CHROM_LENGTH = 10000
Ne = 1000
RECOMBINATION_RATE = 1e-8
MU = 1e-6
SEED = 1337
READ_ERRRATE = 0.002  # q score of 27
ANCESTRAL = "rawdata/lambda/lambda.fasta"


import msprime
import numpy as np
import string
import shutil
import os.path as op


SAMPLES = [f"S{i+1:02d}" for i in range(N_SAMPLES)]
LIBS = {S + l: S for S in SAMPLES for l in string.ascii_lowercase[:LIBS_PER_SAMP]}
RUNS = [f"Run{i+1}" for i in range(RUNS_PER_LIB)]
RUNLIB2SAMP = {(R, L): S for L, S in LIBS.items() for R in RUNS}


def parsefa(filepath):
    seqs = {}
    with open(filepath) as fh:
        seq = []
        seqid = None
        for line in fh:
            if line.startswith(">"):
                if seqid is not None:
                    seqs[seqid] = "".join(x.strip() for x in seq)
                seqid = line[1:].strip().split(" ")[0]
            else:
                seq.append(line)
        if seqid is not None:
            seqs[seqid] = "".join(x.strip() for x in seq)
    return seqs


if ANCESTRAL is not None:
    ANC_GENOME = parsefa(ANCESTRAL)
    if len(ANC_GENOME) != 1:
        raise ValueError("This dumb workflow expects a single contig in the reference genome.")
    ANC_GENOME_SEQID, ANC_GENOME_SEQ = list(ANC_GENOME.items())[0]
    CHROM_LENGTH = len(ANC_GENOME_SEQ)
    print(ANC_GENOME_SEQID, CHROM_LENGTH)

rule all:
    input:
        "rawdata/reference/genome.fa.fai",
        expand("rawdata/genomes/{sample}.fa.fai", sample=SAMPLES),
        expand("rawdata/reads/{run}/{lib}_{R}.fastq.gz", run=RUNS, lib=LIBS, R=["R1", "R2"]),
        "rawdata/rl2s.tsv",
        "rawdata/kraken/Viral",
        "rawdata/kaiju/Viral/kaiju_db_viruses.fmi",
        "rawdata/ncbitax/",
        "rawdata/kaiju/Viral/nodes.dmp",
        "rawdata/kaiju/Viral/names.dmp",
        "rawdata/centrifuge/lambda",

rule ncbitax:
    output:
        directory("rawdata/ncbitax/")
    log:
        "rawdata/ncbitax/get.log"
    shell:
        "(mkdir -p {output} && cd {output} && wget -O taxdump.tar.gz https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz && tar -zxvf taxdump.tar.gz) &>{log}"


rule centrifuge:
    input:
        ref="rawdata/reference/genome.fa",
        nodes="rawdata/kaiju/Viral/nodes.dmp",
        names="rawdata/kaiju/Viral/names.dmp",
        map="rawdata/lambda/centrifuge-map.txt",
    output:
        directory("rawdata/centrifuge/lambda")
    log:
        "rawdata/centrifuge/lambda/build.log",
    shell:
        "( mkdir -p {output} && "
        "centrifuge-build"
        "   --conversion-table {input.map}"
        "   --taxonomy-tree {input.nodes}"
        "   --name-table {input.names}"
        "   {input.ref}"
        "   {output}/lambda"
        ") &> {log}"


rule kaijudb:
    output:
        "rawdata/kaiju/Viral/kaiju_db_viruses.fmi",
        "rawdata/kaiju/Viral/nodes.dmp",
        "rawdata/kaiju/Viral/names.dmp",
    log:
        "rawdata/kaiju/Viral/build.log",
    shell:
            "( mkdir -p {output}"
            " && curl -L https://kaiju.binf.ku.dk/database/kaiju_db_viruses_2020-05-25.tgz"
            "  | tar xzv -C rawdata/kaiju/Viral"
            ") &>{log}"

rule krakendb:
    output:
        directory("rawdata/kraken/Viral"),
    log:
        "rawdata/kraken/build.log",
    shell:
            "(mkdir -p {output}"
            " && curl -L https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20201202.tar.gz"
            "  | tar xzv -C {output}"
            ") &>{log}"

rule faidx:
    input:
        "{path}.fa"
    output:
        "{path}.fa.fai"
    log:
        "{path}.fa.fai.log"
    priority: 10
    shell:
        "samtools faidx {input} &> {log}"

rule metadata:
    output:
        rl2s="rawdata/rl2s.tsv",
    run:
        with open("rawdata/nobackup", "w") as f:
            print("don't back this up", file=f)
        with open(output.rl2s, "w") as file:
            print("run", "library", "sample", "include", "read1_uri", "read2_uri", "interleaved_uri", "single_uri", "qc_type", sep="\t", file=file)
            for (R, L), S in RUNLIB2SAMP.items():
                print(R, L, S, "Y", f"rawdata/reads/{R}/{L}_R1.fastq.gz", f"rawdata/reads/{R}/{L}_R2.fastq.gz", "", "", "nextera", sep="\t", file=file)


rule genomes:
    output:
        reference="rawdata/reference/genome.fa",
        genomes=expand("rawdata/genomes/{sample}.fa", sample=SAMPLES),
        varpos="rawdata/genomes/variantpos.txt",
    run:
        global ANC_GENOME_SEQ
        global ANC_GENOME_SEQID
        # Simulate coalescent sequences
        tree_sequence = msprime.simulate(
                sample_size=N_SAMPLES,
                Ne=Ne,
                length=CHROM_LENGTH,
                recombination_rate=RECOMBINATION_RATE,
                mutation_rate=MU,
                random_seed=SEED)

        # Fill a random sequence for all sites
        np.random.seed(SEED)
        if ANCESTRAL:
            anc_seq = np.array(list(ANC_GENOME_SEQ)).reshape((1, CHROM_LENGTH))
        else:
            anc_seq = np.random.choice(list("ACGT"), size=CHROM_LENGTH).reshape((1, CHROM_LENGTH))
            ANC_GENOME_SEQ = "".join(anc_seq[0, :])
            ANC_GENOME_SEQID = "AncGenome"
        S = np.repeat(anc_seq, repeats=N_SAMPLES, axis=0)
        varpos = []

        # Per variant site, update allele matrix
        nucs = list("ACGT")
        for variant in tree_sequence.variants():
            np.random.shuffle(nucs)
            pos = int(np.round(variant.site.position))
            varpos.append(pos + 1) # 1-based
            for i, gt in enumerate(variant.genotypes):
                S[i, pos] = nucs[gt]
        for i, sname in enumerate(SAMPLES):
            with open(output.genomes[i], "w") as file:
                print(f">{sname}", file=file)
                print("".join(S[i, :]), file=file)

        with open(output.reference, "w") as file:
            print(f">{ANC_GENOME_SEQID}", file=file)
            print(ANC_GENOME_SEQ, file=file)

        if ANCESTRAL:
            ancgtf = op.splitext(ANCESTRAL)[0] + ".gtf"
            if op.exists(ancgtf):
                shutil.copyfile(ancgtf, "rawdata/reference/genome.gtf")

        with open(output.varpos, "w") as file:
            for p in varpos:
                print(p, file=file)


cov_per_run = int(np.ceil(TOTAL_COV / (RUNS_PER_LIB * LIBS_PER_SAMP)))
reads_per_rl = int(np.ceil(cov_per_run * CHROM_LENGTH / (2 * 100)))
rule reads:
    input:
        lambda wc: expand("rawdata/genomes/{sample}.fa", sample=RUNLIB2SAMP[(wc.run, wc.lib)])
    output:
        r1="rawdata/reads/{run}/{lib}_R1.fastq",
        r2="rawdata/reads/{run}/{lib}_R2.fastq",
    log:
        "rawdata/reads/{run}/{lib}.wgsim.log",
    params:
        cov_per_run=cov_per_run,
        reads_per_rl=reads_per_rl,
    shell:
        "wgsim" 
        "   -e 0.005"
        "   -d 500"
        "   -s 100"
        "   -N {params.reads_per_rl}"
        "   -r 0"
        "   -1 100"
        "   -2 100"
        "   {input}"
        "   {output.r1}"
        "   {output.r2}"
        " &> {log}"


rule gzipfq:
    input:
        "{path}.fastq",
    output:
        "{path}.fastq.gz",
    shell:
        "gzip {input}"
