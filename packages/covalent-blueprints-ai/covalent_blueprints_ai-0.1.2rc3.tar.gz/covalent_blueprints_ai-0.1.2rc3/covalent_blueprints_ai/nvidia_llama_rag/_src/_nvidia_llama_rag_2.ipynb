{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NIM - Snowflake RAG example using Covalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import covalent_cloud as cc\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Already Exists.\n",
      "Environment Already Exists.\n",
      "Environment Already Exists.\n",
      "Environment Already Exists.\n"
     ]
    }
   ],
   "source": [
    "cc.create_env(\n",
    "    name=\"llama3-8b-instruct\",\n",
    "    pip=[\"covalent-blueprints>=0.1.0\", \"vllm==0.5.1\", \"torch==2.3.0\"],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "cc.create_env(\n",
    "    name=\"nim-arctic-embed-l\",\n",
    "    pip=[\"covalent-blueprints>=0.1.0\", \"vllm==0.5.1\", \"torch==2.3.0\"],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "cc.create_env(\n",
    "    name=\"nim-nv-rerankqa-mistral-4b-v3-2\",\n",
    "    pip=[\"covalent-blueprints>=0.1.0\", \"vllm==0.5.1\", \"torch==2.3.0\"],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "cc.create_env(\n",
    "    name=\"snowflake\",\n",
    "    pip=[\"snowflake-connector-python\", \"covalent-blueprints>=0.1.0\"],\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utilities for NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_exec = cc.CloudExecutor(\n",
    "    env=\"nim-llama3-8b-instruct\",\n",
    "    memory=\"32GB\",\n",
    "    num_gpus=1,\n",
    "    gpu_type=\"a6000\",\n",
    "    num_cpus=8,\n",
    "    time_limit=\"3 hours\",\n",
    ")\n",
    "\n",
    "\n",
    "@cc.service(executor=llama3_exec, name=\"NIM Llama3 8B Service\")\n",
    "def nim_llama3_8b_service():\n",
    "    \"\"\"Hosts the NIM for 'meta/llama3-8b-instruct'\"\"\"\n",
    "\n",
    "    # Start local server.\n",
    "    pythonpath = \":\".join([\n",
    "        \"/var/lib/covalent/lib\",\n",
    "        \"/usr/local/lib/python3.10/dist-packages\",\n",
    "    ])\n",
    "    start_nims_server(pythonpath)\n",
    "\n",
    "    # Poll server.\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    test_payload = {\n",
    "        \"model\": \"meta/llama3-8b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n",
    "    }\n",
    "    poll_nims_server(url, headers, test_payload)\n",
    "\n",
    "    return {\"url\": url, \"headers\": headers}\n",
    "\n",
    "\n",
    "@nim_llama3_8b_service.endpoint(\"/generate\")\n",
    "def generate(url=None, headers=None, *, prompt=None, messages=None, **kwargs):\n",
    "    \"\"\"Generate a response to a prompt or a list of\n",
    "    conversational messages. Either `prompt` or `messages`\n",
    "    must be provided.\n",
    "\n",
    "    Kwargs:\n",
    "        prompt (str): A prompt to generate a response for.\n",
    "        messages (list): A list of conversational messages.\n",
    "        **kwargs: Additional arguments for the model server.\n",
    "    \"\"\"\n",
    "    if not (prompt or messages):\n",
    "        return \"Please provide a prompt or a list of messages.\"\n",
    "\n",
    "    # Construct request.\n",
    "    payload = {\"model\": \"meta/llama3-8b-instruct\"}\n",
    "\n",
    "    # Handle message or prompt.\n",
    "    if messages:\n",
    "        payload[\"messages\"] = messages\n",
    "    elif prompt:\n",
    "        payload[\"messages\"] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Include any additional kwargs.\n",
    "    for k, v in kwargs.items():\n",
    "        payload[k] = v\n",
    "\n",
    "    # Forward request to local NIM server.\n",
    "    response = requests.post(\n",
    "        url=url, headers=headers, json=payload, timeout=300\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_exec = cc.CloudExecutor(\n",
    "    env=\"nim-arctic-embed-l\",\n",
    "    memory=\"32GB\",\n",
    "    num_gpus=1,\n",
    "    gpu_type=\"a6000\",\n",
    "    num_cpus=8,\n",
    "    time_limit=\"3 hours\",\n",
    ")\n",
    "\n",
    "\n",
    "@cc.service(executor=emb_exec, name=\"NIM Arctic Embedding Service\")\n",
    "def nim_arctic_embed_service():\n",
    "    \"\"\"Hosts the NIM for 'snowflake/arctic-embed-l'\"\"\"\n",
    "\n",
    "    # Start local server.\n",
    "    pythonpath = \":\".join([\n",
    "        \"/var/lib/covalent/lib\",\n",
    "        \"/usr/local/lib/python3.10/dist-packages\",\n",
    "        \"/usr/lib/python3.10/dist-packages\",\n",
    "        \"/app/src\",\n",
    "    ])\n",
    "    start_nims_server(pythonpath)\n",
    "\n",
    "    # Poll server.\n",
    "    url = \"http://localhost:8000/v1/embeddings\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    test_payload = {\n",
    "        \"model\": \"snowflake/arctic-embed-l\",\n",
    "        \"input\": [\"Hello, world!\"],\n",
    "        \"input_type\": \"query\",\n",
    "    }\n",
    "    poll_nims_server(url, headers, test_payload)\n",
    "\n",
    "    return {\"url\": url, \"headers\": headers}\n",
    "\n",
    "\n",
    "@nim_arctic_embed_service.endpoint(\"/get_embedding\")\n",
    "def get_embedding(url=None, headers=None, *, inputs=None, truncate=\"NONE\"):\n",
    "    \"\"\"Obtain the embedding for a given input text\n",
    "    or a list of input texts.\n",
    "\n",
    "    Kwargs:\n",
    "        inputs (str or list): The input text or list thereof.\n",
    "        truncate (str): The truncation strategy to use. Defaults to 'NONE'.\n",
    "    \"\"\"\n",
    "    if not inputs:\n",
    "        return \"Please provide an input text or list thereof.\"\n",
    "\n",
    "    # Construct request.\n",
    "    payload = {\n",
    "        \"model\": \"snowflake/arctic-embed-l\",\n",
    "        \"input\": inputs,\n",
    "        \"input_type\": \"query\",\n",
    "        \"truncate\": truncate,\n",
    "    }\n",
    "\n",
    "    # Forward request to local NIM server.\n",
    "    response = requests.post(\n",
    "        url=url, headers=headers, json=payload, timeout=300\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-ranking NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_exec = cc.CloudExecutor(\n",
    "    env=\"nim-nv-rerankqa-mistral-4b-v3\",\n",
    "    memory=\"32GB\",\n",
    "    num_gpus=1,\n",
    "    gpu_type=\"a6000\",\n",
    "    num_cpus=8,\n",
    "    time_limit=\"3 hours\",\n",
    ")\n",
    "\n",
    "\n",
    "@cc.service(executor=rr_exec, name=\"NIM RerankQA Service\")\n",
    "def nim_rerankqa_service():\n",
    "    \"\"\"Hosts the NIM for 'nvidia/nv-rerankqa-mistral-4b-v3'\"\"\"\n",
    "\n",
    "    # Start local server.\n",
    "    pythonpath = \":\".join([\n",
    "        \"/var/lib/covalent/lib\",\n",
    "        \"/usr/local/lib/python3.10/dist-packages\",\n",
    "        \"/usr/lib/python3.10/dist-packages\",\n",
    "        \"/app/src\",\n",
    "    ])\n",
    "    start_nims_server(pythonpath)\n",
    "\n",
    "    url = \"http://localhost:8000/v1/ranking\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    test_payload = {\n",
    "        \"query\": {\"text\": \"which way should i go?\"},\n",
    "        \"passages\": [\n",
    "            {\"text\": \"going left has only a 32 percent success rate on Tuesdays\"},\n",
    "            {\"text\": \"every Wednesday, going right is an extra 26% more likely to succeed\"},\n",
    "            {\"text\": \"it's a long way to the top if you wanna rock and roll\"},\n",
    "            {\"text\": \"any way you want it, that's the way you need it\"},\n",
    "        ]\n",
    "    }\n",
    "    poll_nims_server(url, headers, test_payload)\n",
    "\n",
    "    return {\"url\": url, \"headers\": headers}\n",
    "\n",
    "\n",
    "@nim_rerankqa_service.endpoint(\"/ranking\")\n",
    "def ranking(url=None, headers=None, *, query=None, passages=None):\n",
    "    \"\"\"Rank a list of passages for a given query.\n",
    "\n",
    "    Kwargs:\n",
    "        query (dict): The query to rank passages for.\n",
    "        passages (list): The list of passages to rank.\n",
    "    \"\"\"\n",
    "    if not (query and passages):\n",
    "        return {\"error\": \"Missing query or passages\"}\n",
    "\n",
    "    # Handle simple format for ease of use.\n",
    "    if isinstance(query, str):\n",
    "        query = {\"text\": query}\n",
    "\n",
    "    for i, passage in enumerate(passages):\n",
    "        if isinstance(passage, str):\n",
    "            passages[i] = {\"text\": passage}\n",
    "\n",
    "    # Construct request.\n",
    "    payload = {\n",
    "        \"model\": \"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "        \"query\": query,\n",
    "        \"passages\": passages,\n",
    "    }\n",
    "\n",
    "    # Forward request to local NIM server.\n",
    "    response = requests.post(\n",
    "        url=url, headers=headers, json=payload, timeout=300\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_ex = cc.CloudExecutor(\n",
    "    env=\"snowflake\",\n",
    "    num_cpus=4,\n",
    "    memory=\"12GB\",\n",
    "    time_limit=\"3 hours\",\n",
    ")\n",
    "\n",
    "@cc.service(executor=micro_ex, name=\"NIM RAG Interface\")\n",
    "def interface_service(\n",
    "    llama_client, emb_client, rr_client,\n",
    "    sf_user, sf_password, sf_account,\n",
    "    warehouse_name, database_name, schema_name, table_name\n",
    "):\n",
    "    \"\"\"Interface for the multi-NIM network.\"\"\"\n",
    "\n",
    "    import snowflake.connector\n",
    "\n",
    "    conn = snowflake.connector.connect(  # trial account\n",
    "        user=sf_user or os.environ[\"SNOWFLAKE_USER\"],\n",
    "        password=sf_password or os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        account=sf_account or os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    )\n",
    "    # Set up the Snowflake DB.\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"CREATE WAREHOUSE IF NOT EXISTS {warehouse_name}\")\n",
    "    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    cursor.execute(f\"USE DATABASE {database_name}\")\n",
    "    cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    cursor.execute(f\"USE SCHEMA {database_name}.{schema_name}\")\n",
    "\n",
    "    # Create the table if it does not exist.\n",
    "    cursor.execute(\n",
    "        f\"CREATE OR REPLACE TABLE {table_name} \"\n",
    "        f\"(text STRING, embedding VECTOR(float, 1024))\"\n",
    "    )\n",
    "    return {\n",
    "        \"conn\": conn,\n",
    "        \"llama_client\": llama_client,\n",
    "        \"emb_client\": emb_client,\n",
    "        \"rr_client\": rr_client,\n",
    "        \"table_name\": table_name,\n",
    "    }\n",
    "\n",
    "\n",
    "@interface_service.endpoint(\"/ingest_data\")\n",
    "def ingest_data(conn, emb_client, table_name, *, data):\n",
    "    \"\"\"Compute the embedding and ingest data into the\n",
    "    Snowflake DB.\n",
    "\n",
    "    Kwargs:\n",
    "        data (str or list): The text data to ingest.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "\n",
    "    # Obtain embeddings from the Arctic Embedding service.\n",
    "    outputs = emb_client.get_embedding(inputs=data)\n",
    "    embeddings = [output[\"embedding\"] for output in outputs['data']]\n",
    "\n",
    "    # Insert data and embeddings into the Snowflake DB.\n",
    "    cursor = conn.cursor()\n",
    "    for text, embedding in zip(data, embeddings):\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {table_name}(text, embedding) \"\n",
    "            f\"SELECT '{text}', {embedding}::VECTOR(FLOAT, 1024)\"\n",
    "        )\n",
    "    return {\"ingested\": data, \"embeddings\": embeddings}\n",
    "\n",
    "\n",
    "@interface_service.endpoint(\"/query_llama\")\n",
    "def query_llama(\n",
    "    conn, emb_client, rr_client, llama_client,\n",
    "    *,\n",
    "    prompt=None, messages=None, retrieve=True, rerank=True, k=4, n=2\n",
    "):\n",
    "    \"\"\"Query the Llama-RAG-reranking pipeline with a prompt\n",
    "    or a list of messages.\n",
    "\n",
    "    Kwargs:\n",
    "        prompt (str): A prompt to query the pipeline with.\n",
    "        messages (list): A list of messages to query the pipeline with.\n",
    "        retrieve (bool): Whether to retrieve texts from the DB.\n",
    "        rerank (bool): Whether to re-rank the retrieved texts.\n",
    "        k (int): The maximum number of texts to retrieve from the DB.\n",
    "        n (int): The number of top reranked texts to return.\n",
    "    \"\"\"\n",
    "    if not (prompt or messages):\n",
    "        return \"Please provide a prompt or a list of messages.\"\n",
    "\n",
    "    #--# Basic LLM #--#\n",
    "    if not retrieve:\n",
    "        # Generate a response using the Llama3 model.\n",
    "        return llama_client.generate(prompt=prompt, messages=messages)\n",
    "\n",
    "    #--# Complete RAG #--#\n",
    "    # Obtain the query embedding.\n",
    "    if messages:\n",
    "        inputs = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"]\n",
    "    else:\n",
    "        inputs = [prompt]\n",
    "\n",
    "    embeddings = emb_client.get_embedding(inputs=inputs)\n",
    "    embeddings = embeddings['data']\n",
    "\n",
    "    # Retrieve the top k texts from the Snowflake DB.\n",
    "    retrieved = []\n",
    "    cursor = conn.cursor()\n",
    "    for embedding_data in embeddings:\n",
    "        embedding = embedding_data['embedding']\n",
    "        cursor.execute(\n",
    "            \"SELECT text, \"\n",
    "            f\"VECTOR_COSINE_SIMILARITY(embedding, {embedding}::VECTOR(FLOAT, 1024)) \"\n",
    "            \"AS similarity FROM nims_rag_table \"\n",
    "            \"ORDER BY similarity DESC \"\n",
    "            f\"LIMIT {k}\"\n",
    "        )\n",
    "        outputs = [t[0] for t in cursor.fetchall()]\n",
    "        retrieved.extend([output.strip('\"') for output in outputs])\n",
    "\n",
    "    if rerank:\n",
    "        # Re-rank and get the the top n <= k texts.\n",
    "        query_prompt = messages[-1][\"content\"] if messages else prompt\n",
    "        outputs = rr_client.ranking(query=query_prompt, passages=retrieved)\n",
    "        ranked = [retrieved[o[\"index\"]] for o in outputs[\"rankings\"][:n]]\n",
    "        retrieved = ranked\n",
    "\n",
    "    retrieved = \"- \" + \"\\n- \".join(retrieved)\n",
    "    query_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Generate a brief response based on \"\n",
    "                f\"the following information:\\n{retrieved}\"\n",
    "            )\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    if messages:\n",
    "        query_messages.extend(messages)\n",
    "    else:\n",
    "        query_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    return llama_client.generate(messages=query_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "\n",
    "@ct.lattice(executor=micro_ex, workflow_executor=micro_ex)\n",
    "def nims_rag_setup_workflow(\n",
    "    warehouse_name, database_name, schema_name, table_name,\n",
    "    user, password, account,\n",
    "):\n",
    "\n",
    "    llama_client = nim_llama3_8b_service()\n",
    "    emb_client = nim_arctic_embed_service()\n",
    "    rr_client = nim_rerankqa_service()\n",
    "\n",
    "    rag_client = interface_service(\n",
    "        llama_client, emb_client, rr_client,\n",
    "        user, password, account,\n",
    "        warehouse_name, database_name, schema_name, table_name\n",
    "    )\n",
    "\n",
    "    return rag_client, llama_client, emb_client, rr_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_id = cc.dispatch(nims_rag_setup_workflow)(\n",
    "    user=\"YOURUSERNAME\",\n",
    "    password=\"yourpassword\",\n",
    "    account=\"ORGID-USERID\",\n",
    "    warehouse_name=\"nim_rag_warehouse\",\n",
    "    database_name=\"nim_rag_database\",\n",
    "    schema_name=\"nim_rag_schema\",\n",
    "    table_name=\"nims_rag_table\",\n",
    ")\n",
    "print(dispatch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cc.get_result(dispatch_id, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covalent-blueprints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
