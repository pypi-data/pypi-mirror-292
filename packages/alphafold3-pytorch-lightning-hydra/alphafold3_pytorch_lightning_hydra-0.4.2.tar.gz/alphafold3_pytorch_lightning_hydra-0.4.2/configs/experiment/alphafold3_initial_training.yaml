# @package _global_

# lists the experiment parameters corresponding to "Initial training" in Table 6 of the paper

# to execute this experiment run:
# python train.py experiment=alphafold3_initial_training

defaults:
  - override /data: pdb
  - override /model: alphafold3
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["pdb", "alphafold3", "initial_training"]

seed: 12345

trainer:
  min_steps: 1 # prevents early stopping
  max_steps: ${divide:${multiply:1e6, 20}, 256} # NOTE: references values for "Initial training" in Table 6 of the paper
  gradient_clip_algorithm: norm
  gradient_clip_val: 10.0
  # NOTE: here, we have to accumulate gradients independently for each rank, per https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients
  accumulate_grad_batches: ${validate_gradient_accumulation_factor:${data.batch_size}, ${trainer.devices}, ${trainer.num_nodes}}

model:
  optimizer:
    lr: 1.8e-3
  net:
    dim_atom_inputs: 3
    dim_template_feats: 44
    pdb_training_set: true
  # training parameters
  compile: false
  skip_invalid_gradient_updates: true
  # model parameters
  parameters_initialized_from: random
  masked_diffusion_loss_for_non_protein_in_disorder: false
  train_structure_and_distogram: true
  train_pae_head: false
  polymer_ligand_bond_loss_weight: 0.0
  diffusion_add_smooth_lddt_loss: false
  diffusion_add_bond_loss: false

data:
  sample_type: default
  contiguous_weight: 0.2
  spatial_weight: 0.4
  spatial_interface_weight: 0.4
  crop_size: 384
  sampling_weight_for_disorder_pdb_distillation: 0.02
  train_on_transcription_factor_distillation_sets: false
  pdb_distillation: null
  max_number_of_chains: 20
  batch_size: 256

logger:
  wandb:
    tags: ${tags}
    group: "alphafold3-initial-training"
  aim:
    experiment: "alphafold3"
