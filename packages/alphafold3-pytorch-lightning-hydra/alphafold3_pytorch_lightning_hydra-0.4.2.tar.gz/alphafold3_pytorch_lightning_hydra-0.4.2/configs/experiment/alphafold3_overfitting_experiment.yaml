# @package _global_

# lists the experiment parameters corresponding to an "Overfitting Experiment" with a small number of training examples

# to execute this experiment run:
# python train.py experiment=alphafold3_overfitting_experiment

defaults:
  - override /data: pdb
  - override /model: alphafold3
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["pdb", "alphafold3", "overfitting_experiment"]

seed: 12345

trainer:
  min_steps: null
  max_steps: -1
  min_epochs: 1 # prevents early stopping
  max_epochs: 20000
  gradient_clip_algorithm: norm
  gradient_clip_val: 10.0
  # NOTE: here, we have to accumulate gradients independently for each rank, per https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients
  accumulate_grad_batches: ${validate_gradient_accumulation_factor:${data.batch_size}, ${trainer.devices}, ${trainer.num_nodes}}

model:
  optimizer:
    lr: 1e-4
  net:
    dim_atom_inputs: 3
    dim_template_feats: 44
    pdb_training_set: true
  # training parameters
  compile: false
  skip_invalid_gradient_updates: true
  # model parameters
  parameters_initialized_from: random
  masked_diffusion_loss_for_non_protein_in_disorder: false
  train_structure_and_distogram: true
  train_pae_head: false
  polymer_ligand_bond_loss_weight: 0.0
  diffusion_add_smooth_lddt_loss: true
  diffusion_add_bond_loss: true

data:
  sample_type: default
  contiguous_weight: 0.2
  spatial_weight: 0.4
  spatial_interface_weight: 0.4
  crop_size: 5120
  max_msas_per_chain: 128
  sampling_weight_for_disorder_pdb_distillation: 0.02
  train_on_transcription_factor_distillation_sets: false
  pdb_distillation: null
  max_number_of_chains: 20
  batch_size: 256

logger:
  wandb:
    tags: ${tags}
    group: "alphafold3-overfitting-experiment"
  aim:
    experiment: "alphafold3"
