.. Track Analyzer - Quantification and visualization of tracking data
    Authors: Arthur Michaut                                                
    Copyright 2016-2019 Harvard Medical School and Brigham and             
                             Women's Hospital                              
    Copyright 2019-2022 Institut Pasteur and CNRSâ€“UMR3738                  
    See the COPYRIGHT file for details                                     
                                                                           
    This file is part of Track Analyzer package.                           
                                                                           
    Track Analyzer is free software: you can redistribute it and/or modify 
    it under the terms of the GNU General Public License as published by   
    the Free Software Foundation, either version 3 of the License, or      
    (at your option) any later version.                                    
                                                                           
    Track Analyzer is distributed in the hope that it will be useful,      
    but WITHOUT ANY WARRANTY; without even the implied warranty of         
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the           
    GNU General Public License for more details .                          
                                                                           
    You should have received a copy of the GNU General Public License      
    along with Track Analyzer (COPYING).                                   
    If not, see <https://www.gnu.org/licenses/>.   


**********
Quickstart
**********


=================
Data requirements
=================

The mandatory input file of **Track Analyzer** is a data table (a csv or txt file) of tracks containing the position coordinates (in 2D or 3D) along time and the tracks identifiers. 
Optionally, data can be plotted on the original image provided as a 3D or 4D tiff stack (ie. 2D+time or 3D+time). If the format of your movie is 
different (list of images), please convert it to tiff stack using `Fiji <https://fiji.sc/>`_ for instance. 

The position file must contain columns with the x, y, (z) positions, a frame column and track id column. The positions coordinates can be in 
pixels or in scaled data. The information about the scaling and other metadata such as time and length scales will be provided by the user through the graphical interface.

If **Track Analyzer** is run through the Jupyter notebook (see below). The position file format is flexible and the user will be asked to interactively identify all the mandatory columns. 
You can specify that the position file was generated by `Trackmate <https://imagej.net/plugins/trackmate/>`_. The columns are then automatically identified. 

If **Track Analyzer** is run through Galaxy (see below). The position file format has to strictly follow the default column names: x, y, (z), frame, track. 
However, if  the position file was generated by `Trackmate <https://imagej.net/plugins/trackmate/>`_, then the columns are automatically identified. 

If `Trackmate <https://imagej.net/plugins/trackmate/>`_ was used to track data, the csv file that must be used is the one generated by clicking on the "spots" button. 

If **Track Analyzer** is run using command lines (see below), the data directory must contain: 

- a comma-separated csv file named positions.csv which column names are: x, y, (z), frame, track
- a text file named info.txt containing the metadata (see example)
- (optional) a tiff file named stack.tif
- (optional) configuration files in a config directory
The default config files can be generated by running :code:`TA_config <path_to_directory>`. The config files are csv files that can be easily edited. 


====================
Running the pipeline
====================

There are three ways of running **Track Analyzer**. Two user-friendly versions are available: an installation-free web-based tool run on `Galaxy <https://galaxyproject.org/>`_, and full version run on a user-friendly Jupyter notebook. On both versions, **Track Analyzer** can be run without any programming knowledge using its graphical interface. The full version interface is launched by running a `Jupyter notebook <https://jupyter.org/>`_ containing widgets allowing the user to load data and set parameters without writing any code. **Track Analyzer** can also directly be run using command lines (if you need to run if on a distant machine such as a cluster). 

Using Galaxy (recommended for a first trial)
============================================

The installation-free online version is available `here <https://galaxy.pasteur.fr/root?tool_id=toolshed.pasteur.fr/repos/rplanel/track_analyzer/track-analyzer/0.1.0>`_. It is run on the web-base platform `Galaxy <https://galaxyproject.org/>`_, which is easy to use (some documentation regarding Galaxy is available `here <https://training.galaxyproject.org/training-material/>`_). This online version is slightly limited compared to the full version run on Jupyter notebook. Jupyter notebook offers 3D visualization and hand-drawing data selection using a `Napari <https://napari.org/>`_ viewer. Moreover, loaded data are computed step by step throughout the pipeline, which provides the user with better interactivity with the data. Conversely, on Galaxy, the user needs to enter numerical parameters before the analysis can be run. 

Complete documentation about Galaxy is available `here <https://training.galaxyproject.org/training-material/>`_. Here's a quick overview of Galaxy's interface. 

  .. image:: ../_static/screenshots/galaxy_help.jpeg
    :align: center

1. Upload your data to Galaxy. If you want to keep track of your history of analysis, you can create a user account.
2. Choose your input files that were previously uploaded.
3. Enter the parameters necessary to your analysis.
4. Hit the execution button to launch the execution on Galaxy's cluster.
5. You can find in the history panel all the output of each analysis job. For each of the output elements, you can have a quick look (6), or save it (7). Note that when you display output plots, it is not very intuitive how to display again the main interface. The circular arrow 'Run this job again' button (8) displayed on every log file, is then useful. If you press the 'Run this job again' button, the interface will be displayed with the exact same set of parameters as the corresponding job.

Once you excute the job, new boxes appear in history panel (5). There are one box for each type of outputs (csv files, image files, log file, config files etc.). While the job is running these boxes are yellow, once the job is done, they turn green. If there was an error during the execution, the boxes turn red. If you have an error, have a look at the log file, there might be a problem with the data you provided. If you suspect there is an issue with **Track Analyzer**, do not hesitate to report it on our `Gitlab <https://gitlab.pasteur.fr/track-analyzer/track-analyzer>`_. 

Using a Jupyter notebook (recommended for advanced options)
===========================================================

The full version can be run using a Jupyter notebook. Documentation about Jupyter notebooks can be found `here <https://jupyter.org/>`_. Briefly, a notebook comprises a series of 'cells' which are blocks 
of Python code to be executed. Each cell can be run by pressing Shift+Enter.
Each cell will execute a piece of code generating the pipeline graphical interface. They all depend on each other, therefore, they MUST be run in order.
By default, the code of each cell is hidden but it can be shown by pressing the button at the top of the notebook: 'Click here to toggle on/off the raw code'.
Once the code is hidden, you might miss a cell. This is a common explanation if you get an error. If this happens, start the pipeline again a couple of cells above.

To launch a notebook: 

- the notebook is at the root of the git repository, or you can just download it here: 
  :download:`run_TA.ipynb <../_static/run_TA.ipynb>` .
- go to the project folder, or where you downloaded the notebook by running :code:`cd <path_to_the_project_folder>` in a terminal
- activate the environment: run :code:`conda activate <env_name>` (see :ref:`installation<installation>` for more details)
- launch a Jupyter notebook, run :code:`jupyter notebook`
- a web browser opens, click on :code:`run_TA.ipynb`
- to shut down the notebook, press CTRL+C in the terminal. 


Using command lines (only if you need to run it on a distant machine)
====================================================================

If you need to run **Track Analyzer** from a terminal without any graphical interface, it is possible, but you won't beneficiate from the interactive modules. Data filtering and analysis parameters will need to be passed through config files (see examples). **Track Analyzer** comes with four commands:

- :code:`traj_analysis` which runs the trajectory analysis section (see below). 
  It takes as arguments: :code:`path_to_data_directory` (optional: use the flag :code:`-r` or :code:`--refresh` to refresh the database)
- :code:`map_analysis` which runs the map analysis section (see below).
  It takes as arguments: :code:`path_to_data_directory` (optional: use the flag :code:`-r` or :code:`--refresh` to refresh the database)
- :code:`TA_config` which generates default config files. It takes as one argument: :code:`path_to_data_directory`
- :code:`TA_synthetic` which generates synthetic data. It takes as arguments: :code:`path_to_data_directory` (optional: use the flag :code:`-p` or :code:`--plot` to plot the positions as a tiff file).

For all these commands, you can show the help by adding the optional flag :code:`-h` or :code:`--help`. 


==================
Analysis procedure
==================
 
**Track Analyzer** contains a data filtering section and three main analysis sections. This section describes the procedure for the full version, but the installation-free version on Galaxy is very similar, only a few options aren't available. 

Load data
=========

Just follow the instructions on the graphical interface (on the Jupyter notebook or Galaxy), to load your data files.

  .. image:: ../_static/screenshots/loading_data.png
    :align: center

If you run **Track Analyzer** for the first time, enter the metadata.

  .. image:: ../_static/screenshots/setting_metadata.png
    :align: center

If **Track Analyzer** is run through the Jupyter notebook, you can additionally select custom columns of variables you might want to plot. You can then have to type in their names and units to be displayed on the plots. 

You can also set some plotting parameters such as image file format, colors to be used, image resolution, etc. 

  .. image:: ../_static/screenshots/ploting_parameters.png
    :align: center


Data filtering section
======================

Subsets of the datasets can be filtered on spatiotemporal criteria: x, y, z positions, time subset and track duration.
A drawing tool also offers the possibility to hand-draw regions of interest. 

  .. image:: ../_static/screenshots/filter_subsets-1.png
    :align: center
  .. image:: ../_static/screenshots/filter_subsets-2.png
    :align: center

Additionally, specific trajectories can be selected by using their position in a region of interest at a specific time. This feature can be
useful to inspect either their past (back-tracking) or their future (fate-mapping). Trajectories can also be selected just using their ids.

These subsets can then be analyzed separately. The analysis will be run independently on each of them.
Alternatively, they can be analyzed together. Trajectories and computed quantities will then be plotted together using color-coding. 

  .. image:: ../_static/screenshots/subset_naming.png
    :align: center


Trajectory analysis section
===========================

Trajectories can be plotted over the original image, frame by frame, with some custom color-coding (z color-coded, t color-coded, subset, or random). 
The total trajectories can also be plotted together with the option to center their origin. This can be useful to detect some patterns in trajectories. 

Several quantities can be computed and plotted: velocities and acceleration (spatial components and their modulus). 
The local cell density can be estimated by performing a Voronoi tesselation. The Voronoi diagram can be plotted and the area of each Voronoi cell can 
be calculated and plotted. Currently, only the Voronoi tesselation in 2D (even if the data are 3D) is available.
If **Track Analyzer** is run through the Jupyter notebook, you can also plot other variables you selected.  

All these quantities can also be averaged over the whole trajectory and plotted. 

Trajectories can also be quantified using the Mean Squared Displacement (MSD) analysis. The MSD can be plotted and fitted with some diffusion models 
to compute the diffusion coefficient.

  .. image:: ../_static/screenshots/trajectory_section.png
    :align: center


Map analysis section
====================

Data can be averaged on a regular grid to produce maps of such quantities. Two kinds of maps can be plotted: vector fields and scalar fields.

Vector fields
-------------

Velocity and acceleration vectors can be plotted on 2D maps. If 3D data, the z dimension can be color-coded. 
Such maps can be superimposed on a scalar field. 

Scalar fields
-------------

The velocity and acceleration components and moduli can be plotted as color-coded maps. The vector average moduli can also be computed. 
The difference between the velocity mean and the vector average modulus is that the velocity mean is the mean over all velocities in
the grid unit, while the vector average modulus is the modulus of the vector averaged in the grid unit. 
Divergence (contraction and expansion) maps, and curl (rotation) maps can also be plotted.

  .. image:: ../_static/screenshots/map_section.png
    :align: center

Notes on running times
======================

Some analysis steps can be time intensive. In particular, the database pre-processing (splitting and interpolation of tracks, velocity and acceleration calculation, etc.) and the trajectory plotting are the most time-intensive steps. To give ideas about orders of magnitude, the pre-processing step takes about 1 minute to run for a 10\ :sup:`4`-tracked objects dataset on a standard laptop. 


Comparator section
==================

Previously generated data by the trajectory analysis section can be compared by plotting parameters together on the same plots. 


======
Output
======

**Track Analyzer** generates several files, plots, data points, and configuration files. 

Database and configuration files
================================

Some files that are necessary to the processing are generated when the pipeline is executed: 

- *data_base.p* is a binary collection of python objects generated when the initial tracking file is loaded. It allows the initial loading to be skipped if the pipeline is run several times on the same tracking data. It can be refreshed if necessary.
- *info.txt* is a text file containing important metadata: 'lengthscale', 'timescale', 'z_step', 'image_width', 'image_height', 'length_unit', 'time_unit', 'table_unit', 'separator'. It can be interactively generated using the notebook.
- if the original image stack is 4D (3D+t), a `stack_maxproj.tif` is generated by performing a maximum projection over the z dimension, so a 2D image can be used for 2D based plotting


Data output
===========

The trajectory analysis and the map analysis are generated respectively in a *traj_analysis* and *map_analysis* directory. Each subset's analysis is saved in a new folder. 

In each subset's directory:  

- a *config* folder is generated with the configuration parameters used for this specific analysis
- *all_data.csv* stores the subset's table of positions
- *track_prop.csv* stores the averaged quantities along trajectories
- each plot is saved using an image format, size and resolution that can be customized. Additionally, the default colors and color maps can be customized in the plotting parameters sections.
- the data points of each plot is saved in a csv file with the same name as the image file, so you can replot the data using your favorite plotting software


========
Examples
========

Real data
=========

You can get familiar with **Track Analyzer** by running it on example data. For instance, you can analyze data of a C. elegans developing embryo provided by `the cell tracking challenge <http://celltrackingchallenge.net/3d-datasets/>`_. Download the data directory containing trajectories and metadata (these positions were extracted following napari's tutorial):  

:download:`cell tracking challenge <../_static/example/Fluo-N3DH-CE.tar.gz>`

Additionally, you can download the original timelapse for optimal visualization. Download the `dataset <http://data.celltrackingchallenge.net/training-datasets/Fluo-N3DH-CE.zip>`_. And run the following :download:`python script <../_static/example/load_tracking.py>` to extract the image and generate a single tiff file that you can use during the analysis.  
To run the script, open a terminal and run: ::

    pip install imagecodecs
    cd <path_to_script_folder>
    python load_tracking.py <path_to_dataset_folder>

You can also generate the positions by adding the flag :code:`-p`, it will generate the *positions.csv* file present in the archive. 

Warning: if you try to open the generated tiff file with `Fiji <https://fiji.sc/>`_, you will see that the t and z dimensions are not separated. You will have to run "stack to hyperstack" with z=35 and t=195. But this is only if you want to see the file in Fiji, you don't need to do this for **Track Analyzer**!


Synthetic data
==============

You can also analyze synthetic data that were generated to ensure that the analysis performed by **Track Analyzer** is correct. You can download several datasets :download:`here <../_static/example/synthetic_data.tar.gz>`. They all have a *param.csv* with the input values for each trajectory.
You can generate such datasets using the commande line :code:`TA_synthetic data_dir -p` where :code:`data_dir` is the path to the directory where you want to save the data, and :code:`-p` is a flag used to plot the positions and save it to a tif file. A configuration file config.csv is used to generate the tracks, as shown in the example datasets. Essentially, the number of tracks, the number of frames, and their initial postions can be set. The movements are controlled by a random diffusion field and a velocity field. These fields are polynomial fields in the form of :math:`a(x-b)^p + c(y-d)^q + e(z-f)^r + cst`. The program outputs a positions.csv file, a stack.tif file (if the :code:`-p` flag is used), a D_fields.csv file (with the theoretical values of D at each initial position), and a v_fields.csv field (with the theoretical values of the velocity, divergence and curl at each initial position). 


===============
Troubleshooting
===============

- The 3D visualization and the drawing selection tool depend on the `napari <https://napari.org/>`_ package. The installation of this package can lead to issues depending on your system. If you are not able to solve this installation, you will not be able to have access to 3D rendering. However, you will still be able to use **Track Analyzer** without the drawing tool, by using coordinates sliders in the graphical interface.
- The execution of blocks in the Jupyter notebook can be buggy because of the large number of widgets. If you can't normally execute a block by pressing Shift+Enter, use the Execute button at the top of the notebook. 

