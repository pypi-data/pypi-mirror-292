Metadata-Version: 2.1
Name: archon-ai
Version: 0.0.2
Summary: Create and Benchmark LLM Chains with JSON
Home-page: https://github.com/jonsaadfalcon/Archon
Author: Shlok Natarajan
Author-email: shlok.natarajan@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
Requires-Dist: anthropic>=0.31.2
Requires-Dist: datasets>=2.20.0
Requires-Dist: groq>=0.9.0
Requires-Dist: huggingface-hub>=0.24.2
Requires-Dist: loguru>=0.7.2
Requires-Dist: numpy>=2.0.1
Requires-Dist: openai>=1.37.1
Requires-Dist: pandas>=2.2.2
Requires-Dist: plotly>=5.23.0
Requires-Dist: requests>=2.32.3
Requires-Dist: shortuuid>=1.0.13
Requires-Dist: tabulate>=0.9.0
Requires-Dist: tiktoken>=0.7.0
Requires-Dist: together>=1.2.3
Requires-Dist: tokenizers>=0.19.1
Requires-Dist: tqdm>=4.66.4
Requires-Dist: transformers
Requires-Dist: torch

# ðŸš§ðŸš§ðŸš§ Under Construction ðŸš§ðŸš§ðŸš§

Currently the python package is not up to date. You will have to import Archon from this repository. As long as you've done that, everything should work as written below.

I recommend you refer to the README within each evaluation, those are currently up to date.
For example, to run on mt_bench, read `mt_bench/README.md`

Still want to run on your own questions? The below documentation should work as long as your importing from `archon.py`. Want to add your own benchmark? [Benchmarks](#benchmarks) is also up to date. (if you run into some path import errors, please run `gen_answers.py` within the `archon` subdirectory)

# ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§ðŸš§

## Archon - Create and Benchmark LLM Chains with JSON

As the number of large language models available increases, using different models in tandem can provide better results than one alone. Archon provides a modular framework for sampling, ranking, and fusing model responses to surpass the capabilities of existing closed-source APIs.
#### Table of Contents

- [Quick Start](#quick-start)
    - [Archon Installation](#archon-installation)
    - [Multi-Model Setup](#multi-model-setup)
    - [More Examples](#more-examples)
- [Benchmarks](#benchmarks)
    - [Add Your Own Benchmark](#add-your-own-benchmark)
- [Resources](#resources)

## Quick Start 
<a target="_blank" href="https://colab.research.google.com/drive/17EFD6ggW0rk5Qz-vBwOhP9RhCnLYviTt#scrollTo=ymybYfBTt4gu">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

### Archon Installation
Archon is publicly available for use at https://pypi.org/project/archon-ai/. Download and install the latest Archon version.
```
pip install archon-ai
```
Archon configs are instantiated via the class Archon available that can be imported from the repository
```python
from archon import Archon
```
### Basic Example
Here's where the magic happens. Archon works by taking in a config file in JSON format that specifies the model you want to run and its available parameters. 
Let's start with the basics. Say I want to ask GPT 4 Turbo a question and output a singular response. We could create a config that looks like this:
```
archon_config = {
    "name": "gpt-4-turbo-quickstart",
    "layers": [
         [
            {
                "type": "model",
                "model": "gpt-4-turbo",
                "model_type": "OpenAI_API",
                "checkpoint": "",
                "top_k": 1,
                "temperature": 0.7,
                "max_context_length": 2048,
                "samples": 1
            }
        ]
    ]
}
```
To generate a response:
```python
# The config can be read from a .json file or directly from a python dictionary
archon = Archon(archon_config)

testing_instruction = [{"role": "user", "content": "How do I make a cake?"}]

response = archon.generate(testing_instruction)

print(response)
```
### Multi-Model Setup
<a target="_blank" href="https://colab.research.google.com/drive/14ohSRBD9mDympZk0MO0MumWnjA4tCk1e#scrollTo=ZrxjlqILrWla">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

Let's move on to something more complicated. Let's say I want to query Qwen 2 from Together, have Claude 3.5 Sonnet critique the response, and then use both the original response and the critique to generate an improved final output using Qwen 1.5 as the fuser. Here's what our config would look like:
```
archon_config =  {
    "name": "archon-testing",
    "layers": [
        [   
            {
                "type": "model",
                "model": "Qwen/Qwen2-72B-Instruct",
                "model_type": "Together_API",
                "checkpoint": "",
                "temperature": 0.7,
                "max_context_length": 2048,
                "samples": 10
            }
        ],
        [
            {
                "type": "critic",
                "model": "claude-3-5-sonnet-20240620",
                "model_type": "Anthropic_API",
                "checkpoint": "",
                "temperature": 0.7,
                "max_context_length": 8192,
                "samples": 1
            }
        ],
        [
            {
                "type": "fuser",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "model_type": "Together_API",
                "checkpoint": "",
                "top_k": 1,
                "temperature": 0.7,
                "max_context_length": 2048,
                "samples": 1
            }
        ]
    ]
}
```
### More Examples
Under ```archon/configs```, you can find more examples of increasingly complex LLM network-style systems <br />
Two systems we wanted to highlight: <br />
Improving GPT 4o using Archon <br />
<a target="_blank" href="https://colab.research.google.com/drive/14ohSRBD9mDympZk0MO0MumWnjA4tCk1e#scrollTo=ZrxjlqILrWla">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
<br />
Using open-sourced LLMs and Archon to outperform GPT 4o <br />
<a target="_blank" href="https://colab.research.google.com/drive/1mhXNc6xfR6CxrHv_tF2xcr5xzKmnA9nD#scrollTo=ZrxjlqILrWla">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## Benchmarks
Once you have created a config, you can leverage pre-existing benchmark frameworks to assess accuracy. We provide classes for [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval), [ArenaHardAuto](https://github.com/lm-sys/arena-hard-auto), and [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench). The classes are under ```archon/benchmarks.py``` where you can modify the input question files under the class ```load_dataset()``` call.
Here is an example command for running your config against ArenaHardAuto:
```
python3 archon/gen_answers.py --benchmark arena_hard_auto --config archon/configs/<your-config-file>.json --output_dir outputs --parallel 16
```
This will run the model structure specified in your config file against the question set specified under the ArenaHardAuto class and output the responses in `.jsonl` format under an `outputs` folder. 

### Add Your Own Benchmark
To add your benchmark, you must edit the benchmarks.py file and add your benchmark class. The 'Benchmark' class can be used as a base class for interfacing between gen_answers.py and your benchmark. Lastly, make sure to add your evaluation to 'BENCHMARK_CLASSES' so it can be used as an argument in gen_answers.py

## Resources
### Inspiration
- ðŸ“š [PyTorch](https://github.com/pytorch/pytorch/): placeholder
- ðŸ“š [DSPy](https://github.com/stanfordnlp/dspy): placeholder
- ðŸ“š [ARES](https://github.com/stanford-futuredata/ARES.git): placeholder
- ðŸ“š [TextGrad](https://github.com/zou-group/textgrad?tab=readme-ov-file): placeholder

### Citation
```bibtex
@article{placeholder,
      title={Archon: Bending the Scaling Curves with Model Ensembling, Sampling, and Ranking},
      author={placeholder},
      year={2024},
      eprint={placeholder},
      archivePrefix={arXiv}
}
```

