{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model surgery\n",
    "\n",
    "This guide will demostrate how to do model surgery in NNX with a few real-scenario use cases.\n",
    "\n",
    "* __Module manipulation__: Pythonic ways to manipulate submodules given a model.\n",
    "\n",
    "* __Abstact model__: A key trick to play with NNX modules and states without memory allocation.\n",
    "\n",
    "* __From raw state to model__: How to manipulate parameter state when they are incompatible with existing model code.\n",
    "\n",
    "* __Partial initialization__: Initializing only part of the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "from jax import lax, numpy as jnp, tree_util as jtu\n",
    "\n",
    "from jax.sharding import PartitionSpec, Mesh, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "import flax\n",
    "from flax import nnx\n",
    "import flax.traverse_util\n",
    "import numpy as np\n",
    "import orbax.checkpoint as orbax\n",
    "\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP(nnx.Module):\n",
    "  def __init__(self, dim, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(dim, dim, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.linear1(x)\n",
    "    return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic module manipulations\n",
    "\n",
    "Model surgery is easiest when you already have a fully fleshed-out model loaded with correct parameters, and you don't intend to change your model definition code.\n",
    "\n",
    "You can make a variety of pythonic operations on its submodules, like swapping in/out, sharing modules/weights, monkeypatching, etc. See a few code examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "x = jax.random.normal(jax.random.key(42), (3, 4))\n",
    "np.testing.assert_allclose(model(x), model.linear2(model.linear1(x)))\n",
    "\n",
    "# Submodule swapping\n",
    "original1, original2 = model.linear1, model.linear2\n",
    "model.linear1, model.linear2 = model.linear2, model.linear1\n",
    "np.testing.assert_allclose(model(x), original1(original2(x)))\n",
    "\n",
    "# Module sharing (tying all weights)\n",
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "model.linear2 = model.linear1\n",
    "assert not hasattr(nnx.state(model), 'linear2')\n",
    "np.testing.assert_allclose(model(x), model.linear1(model.linear1(x)))\n",
    "\n",
    "# Variable sharing (weight tying)\n",
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "model.linear1.kernel = model.linear2.kernel  # the bias parameter is kept separate\n",
    "assert hasattr(nnx.state(model), 'linear2')\n",
    "assert hasattr(nnx.state(model)['linear2'], 'bias')\n",
    "assert not hasattr(nnx.state(model)['linear2'], 'kernel')\n",
    "\n",
    "# Monkey patching\n",
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "def awesome_layer(x): return x\n",
    "model.linear2 = awesome_layer\n",
    "np.testing.assert_allclose(model(x), model.linear1(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and state without memory allocation\n",
    "\n",
    "For more complex model surgery, a key technique is creating and manipulating an abstract model or state without allocating any real parameter data. This makes trial iteration faster and removes any concern on memory constraints.\n",
    "\n",
    "To create an abstract model,\n",
    "* Create a function that returns a valid NNX model; and\n",
    "* Run `nnx.eval_shape` (not `jax.eval_shape`) upon it.\n",
    "\n",
    "Now you can use `nnx.split` as usual to get its abstract state. Note that all the fields that should be `jax.Array` in a real model is now an abstract `jax.ShapeDtypeStruct` with only shape/dtype/sharding information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State({\n",
      "  'linear1': {\n",
      "    'bias': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4,), dtype=float32)\n",
      "    ),\n",
      "    'kernel': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4, 4), dtype=float32)\n",
      "    )\n",
      "  },\n",
      "  'linear2': {\n",
      "    'bias': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4,), dtype=float32)\n",
      "    ),\n",
      "    'kernel': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4, 4), dtype=float32)\n",
      "    )\n",
      "  }\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "abs_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "gdef, abs_state = nnx.split(abs_model)\n",
    "pprint(abs_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you fill every `VariableState` leaf's `value`s with real jax arrays, the abstract model becomes equivalent to a real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "abs_state['linear1']['kernel'].value = model.linear1.kernel\n",
    "abs_state['linear1']['bias'].value = model.linear1.bias\n",
    "abs_state['linear2']['kernel'].value = model.linear2.kernel\n",
    "abs_state['linear2']['bias'].value = model.linear2.bias\n",
    "nnx.update(abs_model, abs_state)\n",
    "np.testing.assert_allclose(abs_model(x), model(x))  # they are equivalent now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint surgery\n",
    "\n",
    "With the abstract state technique in hand, we can do arbitrary manipulation on any checkpoint (or runtime parameter pytree) to make them fit with our given model code, then call `nnx.update` to merge them.\n",
    "\n",
    "This is helpful when you are to change model code significantly (e.g., migrating from Linen to NNX) so that old weights are no longer naturally compatible. Let's run a simple example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a version of model into a checkpoint\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "old_model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "checkpointer.save(f'/tmp/nnx-surgery-state', nnx.state(model), force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this new model, the submodules are renamed from `linear(1|2)` to `layer(1|2)`. Since the pytree structure changed, it's impossible to load the old checkpoint with the new model state structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will throw error: <class 'KeyError'>: 'layer1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivyzheng/envs/py310/lib/python3.10/site-packages/orbax/checkpoint/type_handlers.py:1401: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class ModifiedTwoLayerMLP(nnx.Module):\n",
    "  def __init__(self, dim, rngs: nnx.Rngs):\n",
    "    self.layer1 = nnx.Linear(dim, dim, rngs=rngs)  # no longer linear1!\n",
    "    self.layer2 = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.layer1(x)\n",
    "    return self.layer2(x)\n",
    "\n",
    "abs_model = nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "try:\n",
    "  with_item = checkpointer.restore('/tmp/nnx-surgery-state', item=nnx.state(abs_model))\n",
    "  print(with_item)\n",
    "except Exception as e:\n",
    "  print(f'This will throw error: {type(e)}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can load the parameter tree as a raw dictionary, make the renames, and generate a new state that is guaranteed to be compatible with your new model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear1': {'bias': {'raw_value': Array([0., 0., 0., 0.], dtype=float32)},\n",
      "             'kernel': {'raw_value': Array([[-0.80345297, -0.34071913, -0.9408296 ,  0.01005968],\n",
      "       [ 0.26146442,  1.1247735 ,  0.54563737, -0.374164  ],\n",
      "       [ 1.0281805 , -0.6798804 , -0.1488401 ,  0.05694951],\n",
      "       [-0.44308168, -0.60587114,  0.434087  , -0.40541083]],      dtype=float32)}},\n",
      " 'linear2': {'bias': {'raw_value': Array([0., 0., 0., 0.], dtype=float32)},\n",
      "             'kernel': {'raw_value': Array([[ 0.21010089,  0.8289361 ,  0.04589564,  0.5422644 ],\n",
      "       [ 0.41914317,  0.84359694, -0.47937787, -0.49135214],\n",
      "       [-0.46072108,  0.4630125 ,  0.39276958, -0.9441406 ],\n",
      "       [-0.6690758 , -0.18474789, -0.57622856,  0.4821079 ]],      dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "def module_from_variables_dict(module_factory, variables, map_key_fn):\n",
    "  if map_key_fn is None:\n",
    "    map_key_fn = lambda path: path\n",
    "  mdl = nnx.eval_shape(module_factory)\n",
    "  graph_def, state = nnx.split(mdl)\n",
    "  state = state.flat_state()\n",
    "  for path, val in flax.traverse_util.flatten_dict(variables).items():\n",
    "    mapped_path = map_key_fn(path)\n",
    "    if mapped_path not in state:\n",
    "      raise ValueError(f\"{mapped_path} doesn't exist in {state.keys()}\")\n",
    "    state[mapped_path].value = val\n",
    "  state = nnx.State.from_flat_path(state)\n",
    "  return nnx.merge(graph_def, state)\n",
    "\n",
    "# Make your local change on the checkpoint\n",
    "raw = checkpointer.restore('/tmp/nnx-surgery-state')\n",
    "pprint(raw)\n",
    "raw['layer1'], raw['layer2'] = raw['linear1'], raw['linear2']\n",
    "del raw['linear1'], raw['linear2']\n",
    "\n",
    "restored_model = module_from_variables_dict(\n",
    "  lambda: nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0))),\n",
    "  raw,\n",
    "  lambda path: path[:-1] if path[-1] == 'raw_value' else path\n",
    ")\n",
    "\n",
    "np.testing.assert_allclose(restored_model(jnp.ones((3, 4))), old_model(jnp.ones((3, 4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial initialization\n",
    "\n",
    "In some cases (e.g., LoRA), you might want to randomly-initialize only *part of* your model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive partial initialization\n",
    "\n",
    "You can simply initialize the whole model, then swap pre-trained params in. But this approach could allocate additional memory midway, if your modification requires re-creating module params that you will later discard. See this example below.\n",
    "\n",
    "> Note: You can use `jax.live_arrays()` to check all the arrays live in memory at any given time. This call can be messed up when you run a single notebook cell multiple times (due to garbage-collecting old python variables), but restarting kernel & running from scratch will always yield same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jax arrays in memory at start: 34\n",
      "Number of jax arrays in memory midway: 38 (4 new created in LoRALinear - kernel, bias, lora_a & lora_b)\n",
      "Number of jax arrays in memory at end: 36 (2 discarded - only lora_a & lora_b are used in model)\n"
     ]
    }
   ],
   "source": [
    "# Some pretrained model state\n",
    "old_state = nnx.state(TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "\n",
    "simple_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(42)))\n",
    "print(f'Number of jax arrays in memory at start: {len(jax.live_arrays())}')\n",
    "# On this line, extra kernel and bias is created inside the new LoRALinear!\n",
    "# They are wasted b/c we are to use the kernel and bias in `old_state` anyway.\n",
    "simple_model.linear1 = nnx.LoRALinear(4, 4, lora_rank=3, rngs=nnx.Rngs(42))\n",
    "print(f'Number of jax arrays in memory midway: {len(jax.live_arrays())}'\n",
    "      ' (4 new created in LoRALinear - kernel, bias, lora_a & lora_b)')\n",
    "nnx.update(simple_model, old_state)\n",
    "print(f'Number of jax arrays in memory at end: {len(jax.live_arrays())}'\n",
    "      ' (2 discarded - only lora_a & lora_b are used in model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-efficient partial initialization\n",
    "\n",
    "Use `nnx.jit`'s efficiently compiled code to make sure only the state params you need are initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jax arrays in memory at start: 40\n",
      "Number of jax arrays in memory at end: 42 (2 new created - lora_a and lora_b)\n"
     ]
    }
   ],
   "source": [
    "# Some pretrained model state\n",
    "old_state = nnx.state(TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "\n",
    "# Use nnx.jit (which wraps jax.jit) to automatically skip unused arrays - memory efficient!\n",
    "@functools.partial(nnx.jit, donate_argnums=0, static_argnums=1)\n",
    "def partial_init(old_state, rngs):\n",
    "  model = TwoLayerMLP(4, rngs=rngs)\n",
    "  # create new state\n",
    "  model.linear1 = nnx.LoRALinear(4, 4, lora_rank=3, rngs=rngs)\n",
    "  # add existing create\n",
    "  nnx.update(model, old_state)\n",
    "  return model\n",
    "\n",
    "print(f'Number of jax arrays in memory at start: {len(jax.live_arrays())}')\n",
    "# Note that `old_state` will be deleted after this `partial_init` call.\n",
    "good_model = partial_init(old_state, nnx.Rngs(42))\n",
    "print(f'Number of jax arrays in memory at end: {len(jax.live_arrays())}'\n",
    "      ' (2 new created - lora_a and lora_b)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
